{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "\n",
    "## From Biological to Artificial Neurons\n",
    "- first introduced in 1943 by neurophysiologist Warren McCulloch and Walter Pitts\n",
    "    - presented a simplified computational model of how biological neurons might work together to perform complex computations using propositional logic\n",
    "- ### Biological Neurons\n",
    "    - An unusual-looking cell found in animal brains\n",
    "    - composed of \n",
    "        - cell body - contains the nucleus and most of the cmoplex components\n",
    "        - Dendrites - branching extensions\n",
    "        - axon - very long extension\n",
    "            - axon splits up into many branches called telodendria\n",
    "                - the tip is the synaptis terminals (synapses) which are connected to the dendrites or cell bodies of other neurons\n",
    "    - Makes short electircal impulses caleed action potentials which makes the synapses release neurotransmitters\n",
    "    - With neurotransmitters within a few milliseconds, a neuron fires its own electrical impulses\n",
    "    - Neurons behave in a simple way but they are interconnected in a network of billions of other neurons, with each neuron connected to thousands of others\n",
    "    - Complex computations can be performed with a network of simple neurons\n",
    "\n",
    "*Biological Neural Networks (BNNs) are still subject to active research*. Although, some parts of the brain have been mapped and it seems that neurons are organized in onsecutive layers\n",
    "\n",
    "- ### Artificial Neuron\n",
    "    - one of more binary inputs and one binary output\n",
    "    - the output is activated when more than a threshold of certain number of inputs are active\n",
    "    - a simple network of artificial neurons can perform complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "- one of the simplest ANN architectures\n",
    "- inveted by Frank Rosenblatt\n",
    "- It is based on a slightly different artificial neuron called *threshold logic units (TLUs)* aka *linear threshold units (LTUs): **the inputs are number rather than binary values, and each input connection is associated with a weight\n",
    "- the TLU computes the weighted sum of its inputs then applies a step function to that sum\n",
    "    - the most common step funciton used in Perceptrons is Heaviside step function (heavisize(z) = 0 if z < 0 else 1 if z >= 0)\n",
    "- composed of a **single** layer of TLUs w/ each TLU connected to all the inputs\n",
    "- **Dense layer:** when all neurons in that layer is connected to every neuron in the previous layer (in this case the input neurons)\n",
    "- all the input neurons form the inputs layer\n",
    "- bias: an extra neuron that represents the bias is added. It always outputs 1\n",
    "\n",
    "equation for the output of a fully connected layer: h(X) = activation_function(XW+b)\n",
    "- X = matrix of input features. one row per instance, one column per feature\n",
    "- W = connection weights excl. bias neuron. one row per input neuron, one column per artificial neuron in the layer\n",
    "- b = matrix of biases. one bias term per artificial neuron\n",
    "\n",
    "### The TLU/LTU\n",
    "- one TLU can be used for simple binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how is a perceptron trained?\n",
    "- The connection between two neurons tends to increase when they fire simultaenously - \"Hebb's rule\"\n",
    "    - perceptrons are made by a variant of this rule that takes into account the error made by the network when it makes a prediction\n",
    "- It is fed one training instance at a time, and for each instance, predicts the label\n",
    "- For every output neuron that produced a wrong predction, the connections to the weights from the inputs that would have contributed to the correction is strengthened/reinforced\n",
    "- training rule: w_next_step = w + learning_rate(target_output - predicted_output)*[ith input val of the current training instance]\n",
    "\n",
    "**perceptrons have a linear decision boundary so it cannot learn complex patterns like logisitc regression does**\n",
    "\n",
    "Perceptron convergence theorem: If the dataset is linearly separable, the algorithm converges to a solution\n",
    "\n",
    "**output of perceptron**: predictions on hard threshold. It does not output probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with single-layer perceptrons: cannot solve exclusive OR classification problems (true for any linear classification model)\n",
    "- **solution**: stack multiple perceptrons together\n",
    "    - Multilayer Perceptron, which can solve XOR problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons (MLPs)\n",
    "- stack of perceptrons\n",
    "- Composed of:\n",
    "    - input layer\n",
    "    - hidden layers - one or more layers of TLUs\n",
    "        - lower layers - layers closer to input layer\n",
    "        - upper layers - layers closer to output layer\n",
    "    - output layer - final layer of TLUs\n",
    "    - bias neuron as every layer except output layer\n",
    "    \n",
    "**Deep learning: when an ANN contains <u>deep</u> stacks of hidden layers**. People say that even shallow NNs are deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there was no way to train MLPs until **backpropagation**\n",
    "- Backpropgation: two passes in the network (one forward, one backward)\n",
    "    - computes the gradient of the network's error with regard to everry single model parameter. Basically it determines how each connection weight and each bias term should be tweaked in order to reduce the error.\n",
    "    - Once it has those gradients, it does a gradient descent step and repeats until it converges.\n",
    "    \n",
    "#### Backpropagation in more detail\n",
    "- handles one mini-batch at a time (e.g. 32 instances each batch) and goes through the full training set multiple times. Each pass is called an *epoch* (one forward + backward pass)\n",
    "- Each mini-batch is passed to the network's input layer, which sends it to the first hidden layer\n",
    "    - The algorithm computes the output of all neurons in the first hidden layer, then passes it to the next layer, until the output layer\n",
    "- **up to this point is called the \"forward pass\"** it is like making prediction\n",
    "- measures the network's output error w/ a loss function\n",
    "- computes how much each output connection contributed to the error using the chain rule (the most fundamental rule in calculus), which makes this step fast and precise\n",
    "- measures how much each connection contributed to the error in the layer below using the chain rule, working backward until the input layer\n",
    "- final step: performs gradient descent step to tweak all the connection weights in the network, using the gradients it just computed\n",
    "\n",
    "#### Backpropagation in summary:\n",
    "- for each training instance:\n",
    "    - the backprop algo first makes a prediction (forward pass) and measures the error\n",
    "- then goes through each layer in reverse to measure the error contribtion from each connection (reverse pass)\n",
    "- tweaks the connection weights to reduce the error (Gradient descent step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize hidden layer's connection weights randomly* **DO NOT WEIGH THEM EQUALLY**:\n",
    "- the backprop algo will treat all neurons equally and their connection weights equally, which goes against the point of backpropagation\n",
    "- Conquer the enemy and break the symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLPs activation functions were changed from the step function to the sigmoid (logistic) *sigmoid(z)=1/(1+exp(-z)* function because it is continuous and enables gradient descent to improve because the sigmoid's derivative is nonzero at every point\n",
    "\n",
    "other activation functions:\n",
    "- hyperbolic tangent tanh(z) = 2sigma(2z)-1\n",
    "    - S-shaped\n",
    "    - output from -1 to 1\n",
    "- Rectified Linear Unit (ReLU) ReLU(z) = max(0,z)\n",
    "    - not differntiable at z=0\n",
    "    \n",
    "#### So what the heck is the point of activation functions??\n",
    "- Since perceptron is basically chaining functions in each layer, those chained functions will still be linear without activation functions.\n",
    "- The point of MLPs is to solve more complex problems, not just linear ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types of MLPs\n",
    "### Regression MLPs\n",
    "- for single value predictions, only need a single output neuron\n",
    "    - for double val preds, will need two output neurons\n",
    "    - etc. \n",
    "    - 1 output neuron per dimension\n",
    "- In general, do not use an activation function for the output neurons so they can output any range of values. Scenarios to use **output activation functions:**\n",
    "    - can use ReLU or softplus to output only positive values\n",
    "    - can use logistic function of hyperbolic tangent to guarantee values fall within a certain range\n",
    "- Loss functions:\n",
    "    - MSE\n",
    "    - MAE\n",
    "    - Huber loss: combination of MSE and MAE\n",
    "    \n",
    "### Classification MLPs\n",
    "- # of neurons:\n",
    "    - for binary classification, use single output neuron using logistic activation function so the ouput will be b/t 0 and 1 (can be interpreted as the estimated probability for the positive class)\n",
    "    - for multi binary classification that predicts if an email is ham or spam AND if its urgent or not:\n",
    "    - use 2 neurons w/ both using logistic activation function (first neuron outputs spam/ham, second outputs urgent/nonurgent). \n",
    "        - probabilities do not have to add up to 1 b/c spam/ham and urgent/nonurgent cases are not mutually exclusive. The outputs of each neuron are not mutually exclusive\n",
    "    - Basically one output neuron for each positive class\n",
    "    - for multiclass classification:\n",
    "        - if each instances only belongs to one class out of three of more possible classes (0-9 for digit img classification) have one output neuron per class and use softmax activation function so that the probabilities are b/t 0 and 1 and add up to 1 b/c they classes are mutually exclusive\n",
    "- loss function:\n",
    "    - it is predicting probability distributions so use cross-entropy (log) loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
