{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Networks Hyperparameters\n",
    "- one of the main drawbacks of NNs is that there are many hyperparameters to tweak\n",
    "\n",
    "#### option 1: try many combinations of hyperparams and see which one works best on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_test_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_test_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]): # provide reasonable defaults to the model\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# sequential model for multivariate regression (one neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wrap the model with KerasRegressor so that it will have the same methods as Scikit-Learn regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now keras_reg will have the *fit(), score(), and predict()* methods like in Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdcd90ee490>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[keras.callbacks.EarlyStopping(patience=10)], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 418us/step - loss: 0.3629\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use RandomizedSearchCV because there are many hyperparameters to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "121/121 [==============================] - 0s 414us/step - loss: 0.3158\n",
      "[CV] END learning_rate=0.002058508271902106, n_hidden=3, n_neurons=14; total time=  15.2s\n",
      "121/121 [==============================] - 0s 416us/step - loss: 0.3490\n",
      "[CV] END learning_rate=0.002058508271902106, n_hidden=3, n_neurons=14; total time=  15.0s\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.3642\n",
      "[CV] END learning_rate=0.002058508271902106, n_hidden=3, n_neurons=14; total time=  15.1s\n",
      "121/121 [==============================] - 0s 410us/step - loss: 0.3412\n",
      "[CV] END learning_rate=0.004818367294934084, n_hidden=1, n_neurons=38; total time=  13.8s\n",
      "121/121 [==============================] - 0s 400us/step - loss: 0.3501\n",
      "[CV] END learning_rate=0.004818367294934084, n_hidden=1, n_neurons=38; total time=  13.9s\n",
      "121/121 [==============================] - 0s 410us/step - loss: 0.3619\n",
      "[CV] END learning_rate=0.004818367294934084, n_hidden=1, n_neurons=38; total time=  14.4s\n",
      "121/121 [==============================] - 0s 386us/step - loss: 0.3151\n",
      "[CV] END learning_rate=0.010812911851930444, n_hidden=1, n_neurons=33; total time=  14.3s\n",
      "121/121 [==============================] - 0s 402us/step - loss: 0.3284\n",
      "[CV] END learning_rate=0.010812911851930444, n_hidden=1, n_neurons=33; total time=  14.3s\n",
      "121/121 [==============================] - 0s 412us/step - loss: 0.3400\n",
      "[CV] END learning_rate=0.010812911851930444, n_hidden=1, n_neurons=33; total time=  14.4s\n",
      "121/121 [==============================] - 0s 412us/step - loss: 0.4291\n",
      "[CV] END learning_rate=0.0003145538090468832, n_hidden=2, n_neurons=62; total time=  15.0s\n",
      "121/121 [==============================] - 0s 417us/step - loss: 0.4548\n",
      "[CV] END learning_rate=0.0003145538090468832, n_hidden=2, n_neurons=62; total time=  14.8s\n",
      "121/121 [==============================] - 0s 408us/step - loss: 0.4297\n",
      "[CV] END learning_rate=0.0003145538090468832, n_hidden=2, n_neurons=62; total time=  14.7s\n",
      "121/121 [==============================] - 0s 404us/step - loss: 0.3383\n",
      "[CV] END learning_rate=0.010103196124324077, n_hidden=1, n_neurons=54; total time=  13.9s\n",
      "121/121 [==============================] - 0s 409us/step - loss: 0.3165\n",
      "[CV] END learning_rate=0.010103196124324077, n_hidden=1, n_neurons=54; total time=  14.1s\n",
      "121/121 [==============================] - 0s 431us/step - loss: 0.3322\n",
      "[CV] END learning_rate=0.010103196124324077, n_hidden=1, n_neurons=54; total time=  14.1s\n",
      "121/121 [==============================] - 0s 413us/step - loss: 0.2922\n",
      "[CV] END learning_rate=0.02408823900687922, n_hidden=2, n_neurons=72; total time=   9.1s\n",
      "121/121 [==============================] - 0s 377us/step - loss: 0.2854\n",
      "[CV] END learning_rate=0.02408823900687922, n_hidden=2, n_neurons=72; total time=   9.1s\n",
      "121/121 [==============================] - 0s 430us/step - loss: 0.3339\n",
      "[CV] END learning_rate=0.02408823900687922, n_hidden=2, n_neurons=72; total time=  10.1s\n",
      "121/121 [==============================] - 0s 416us/step - loss: 0.4020\n",
      "[CV] END learning_rate=0.003079496805431345, n_hidden=2, n_neurons=2; total time=  13.6s\n",
      "121/121 [==============================] - 0s 379us/step - loss: 0.4200\n",
      "[CV] END learning_rate=0.003079496805431345, n_hidden=2, n_neurons=2; total time=  13.2s\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.4166\n",
      "[CV] END learning_rate=0.003079496805431345, n_hidden=2, n_neurons=2; total time=  13.8s\n",
      "121/121 [==============================] - 0s 438us/step - loss: 0.2937\n",
      "[CV] END learning_rate=0.004139776317373108, n_hidden=2, n_neurons=74; total time=  14.8s\n",
      "121/121 [==============================] - 0s 438us/step - loss: 0.3163\n",
      "[CV] END learning_rate=0.004139776317373108, n_hidden=2, n_neurons=74; total time=  15.2s\n",
      "121/121 [==============================] - 0s 411us/step - loss: 0.3134\n",
      "[CV] END learning_rate=0.004139776317373108, n_hidden=2, n_neurons=74; total time=  15.2s\n",
      "121/121 [==============================] - 0s 451us/step - loss: 0.3734\n",
      "[CV] END learning_rate=0.0007133701378362844, n_hidden=2, n_neurons=35; total time=  14.7s\n",
      "121/121 [==============================] - 0s 414us/step - loss: 0.4009\n",
      "[CV] END learning_rate=0.0007133701378362844, n_hidden=2, n_neurons=35; total time=  14.5s\n",
      "121/121 [==============================] - 0s 450us/step - loss: 0.4027\n",
      "[CV] END learning_rate=0.0007133701378362844, n_hidden=2, n_neurons=35; total time=  14.6s\n",
      "121/121 [==============================] - 0s 409us/step - loss: 0.4594\n",
      "[CV] END learning_rate=0.0004641039858438632, n_hidden=2, n_neurons=9; total time=  14.6s\n",
      "121/121 [==============================] - 0s 398us/step - loss: 0.4245\n",
      "[CV] END learning_rate=0.0004641039858438632, n_hidden=2, n_neurons=9; total time=  14.9s\n",
      "121/121 [==============================] - 0s 427us/step - loss: 0.4573\n",
      "[CV] END learning_rate=0.0004641039858438632, n_hidden=2, n_neurons=9; total time=  14.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7fdcc888fd90>,\n",
       "                   param_distributions={'learning_rate': [0.008072582276422957,\n",
       "                                                          0.022999308832844866,\n",
       "                                                          0.00037796389193578384,\n",
       "                                                          0.0005764359101138321,\n",
       "                                                          0.004475815288389972,\n",
       "                                                          0.010119228808704825,\n",
       "                                                          0.006221008037106827,\n",
       "                                                          0.026292307701445267,\n",
       "                                                          0.0008953239050796231,...\n",
       "                                                          0.0003918753218625975,\n",
       "                                                          0.005704682291174898,\n",
       "                                                          0.0018743626329273291,\n",
       "                                                          0.001887350019081551,\n",
       "                                                          0.0005038860287683907,\n",
       "                                                          0.0033788868720783464,\n",
       "                                                          0.003071770503470536,\n",
       "                                                          0.0021907997238896945,\n",
       "                                                          0.009081862313381128,\n",
       "                                                          0.0040975415898762005, ...],\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0,1,2,3],\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist() # makes a list from an inverse continuous random variable\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[keras.callbacks.EarlyStopping(patience=10)], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV does not use *validation_data* because it uses k-fold cross validation for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 72, 'n_hidden': 2, 'learning_rate': 0.02408823900687922}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3038170039653778"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This method of searching for the best hyperparameters will be very inefficient if the hyparameter space is too large**\n",
    "\n",
    "#### option 2: Use these python libraries to optimize hyperparameters:\n",
    "- *the core idea is that good regions should be explored more\n",
    "- Hyperopt\n",
    "- Hyperas, kopt, Talos\n",
    "- Keras Tuner\n",
    "- Scikit-Optimize\n",
    "- Spearmint\n",
    "- Hyperband\n",
    "- Sklearn-Deap\n",
    "\n",
    "#### option 3: Use hyperparamter tuning services from:\n",
    "- Google Cloud AI\n",
    "- Arimo\n",
    "- SigOpt\n",
    "- CallDesk Oscar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sidenotes:\n",
    "- Hyperparameter turning is still an active area of research\n",
    "- Evolutionary algorithms are making a comeback\n",
    "- AutoML suite uses evolutionary algorithms to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning number of hidden layers\n",
    "- *Train the network with increasing layers until it starts overfitting*\n",
    "- MLPs with one hidden layer can still perform good on the most complex problems *if it has enough neurons*\n",
    "- **Although deep networks have a much higher <u>parameter efficiency</u> than shallow ones**\n",
    "    - can model complex function using exponentially fewer neurons while performing better with the same amount of training data\n",
    "    \n",
    "    \n",
    "- Analogy:\n",
    "    - In a task where I draw a forest with two kinds of software...\n",
    "    - The first software does not allow copy and paste\n",
    "        - This is terrible because every detail must be drawn repeatedly, even the leaves and branches\n",
    "    - The second software will allow me to draw a leaf once, copy that leaf, draw a branch, paste the leaf on that branch, draw a tree, and paste the branc+leaf on the tree, and then copy and paste the tree repeatedly to make a forest.\n",
    "- With regards to the analogy, real-world data has this **hierarchical structure**\n",
    "    - Deep NNs automatically take advantage of that\n",
    "        - lower hidden layers draw the leaves, branches, and other things\n",
    "        - higher hidden layers put them all together\n",
    "    - Deep NNs converge faster \n",
    "    - Deep NNs generalize better\n",
    "    - **Transfer Learning** example:\n",
    "        - There is a model trained from faces...\n",
    "        - Task is to make a Deep NN model that recognizes hairstyles\n",
    "        - Rather, than randomly initializing the weights and biases of a Deep NN, use the weights and biases computed by the other model\n",
    "        - This way, the lower-level structures will already be learnt and the model will only have to learn the higher-level structures (hairstyle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Very complex tasks in Deep learning will require dozens of layers (or even hundreds but not fully connected ones) and need a huge amount of training data.\n",
    "    - Although!! I should use a pretrained state-of-the-art network that performs a similar task\n",
    "        - This way training will be faster w/ less data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of neurons per Hidden layer\n",
    "- *Train network with increasing #. of neurons until it starts overfitting*\n",
    "- \\# of neurons in input and output layers is determined by the task\n",
    "    - e.g. MNIST requires 28*28=784 input neurons for each pixel and 10 output neurons for each number\n",
    "- Old practice:\n",
    "    - gradually decrease # of neurons per layer (300 in first layer, 200 in second, 100 in third) b/c low-level features can coalesce into high-level features\n",
    "    - **status: abandoned** b/c same # of neurons/layer works just as well, if not better w/ fewer hyperparameters to tune\n",
    "- **\"Strecth Pants approach\"** method to get the correct number of neurons:\n",
    "    - pick a model with more layers and models than needed and use early stopping and other regularization techniques to prevent overfitting\n",
    "- if a layer is too small, it will not have enough representational power to preserve all the useful information from the inputs\n",
    "    - reason: a layer with 2 neurons can only output 2D data. If it's processing 3D data, information will be lost and the rest of the network will never see it. **Ever.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip: increasing # of layers instead of # of neurons per layer gives more bang for buck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Learning Rate\n",
    "- The most important hyperparameter\n",
    "- Train model with hundreds of iterations w/ very low starting learning rate and gradually increase to very large learning rate\n",
    "    - multiply the learning rate by constant factor at each iteration\n",
    "    \n",
    "#### Picking the right optimizer\n",
    "- Will be visited in Chapter 11\n",
    "\n",
    "#### Tuning batch size\n",
    "- larger batch sizes will enable GPUs to process them efficiently (more instances per second). \n",
    "- Many researcher recommended to use the largest batch size that can fit in GPU RAM\n",
    "- The legend **Yann LeCun** tweeted \"friends don't let friends use mini-batches larger than 32\"\n",
    "    - small batches lead to better models in less training time\n",
    "    \n",
    "#### Picking the activation function\n",
    "- hidden layers: The ReLU activation function will be a good default.\n",
    "- output layers: depends on the task duhhhh\n",
    "\n",
    "#### Tuning the right number of operations\n",
    "- does not need to be tweaked in most cases\n",
    "- just use early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
