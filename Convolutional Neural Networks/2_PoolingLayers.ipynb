{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling layers\n",
    "- goal is to *subsample* (shrink) the input image in order to reduce the computational load, memory usage, and the number of parameters (to limit risk of overfitting)\n",
    "    - while conv. net's goal is to recognize patterns in images\n",
    "- Each neuron in a pooling layer is connected to a limited num. of neurons in the prev layer, must have size, string, and padding type defined just like conv. layers\n",
    "- Difference from conv. layer: Pooling neuron has no weights, all it does is aggregate inputs such as using max or mean functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling:\n",
    "- max pooling layer: most common type of pooling layer\n",
    "- pooling kernel - *does not have weights, just stateless sliding windows*\n",
    "- Only the max input value in each receptive field makes it to the next layer, the others are dropped\n",
    "- <img src=\"images/CNNPoolingLayers.jpeg\" width=400/> pooling layer\n",
    "    - (bottom left) only the max value of the receptive field is propagated onto the next layer\n",
    "    - stride of 2 will make the output image (top layer) have half the height and width as the previous layer\n",
    "- introduces some levels of *invariance*\n",
    "    - Makes the system invariant to translatation, it can still detect objects even if they are moved to different places in the image\n",
    "    - Useful in tasks where the ouput does not depend on the position such as classification\n",
    "    - **max pooling introduces a a large amount of translational invariance, a small amount of rotational invariance and a slign scale invariance**\n",
    "- downsides:\n",
    "    - information is lost\n",
    "        - e.g. a 2x2 kernel (receptive field?) and a stride of 2 will out a 75% loss of input values\n",
    "    - invariance is not desirable in some cases\n",
    "        - e.g. semantic segmentation (classifying each pixel in an image acording to the objet that pixel belongs to\n",
    "        - In this case, *equivariance* (a small change to the inputs should lead to a corresponding small change in the output) is the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "max_pool = keras.layers.MaxPool2D(pool_size=2) #pool_size is like receptive field. Here it is a 2x2 window to pick the max value from. divides each \n",
    "# spatial dimension by a factor of two\n",
    "average_pool = keras.layers.AveragePooling2D(pool_size=2) # computes mean rather than max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might be surprising but max pooling generally performs than avg pooling better because it only keeps the most important information, has strongers translation invariance, and slightly fewer computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth-wise max pooling layer\n",
    "- allows the CNN to be invariant to various features (e.g. rotation of a number) and ensure that the output of the same regardless of the rotation\n",
    "- <img src=\"images/CNNDepthwiseMaxPool.jpeg\" width=400/> figure\n",
    "- Keras has no depthwise max pooling layer, but TF does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_sample_image\n",
    "import numpy as np\n",
    "\n",
    "china = load_sample_image(\"china.jpg\") / 255\n",
    "flower = load_sample_image(\"flower.jpg\") / 255\n",
    "images = np.array([china, flower])\n",
    "\n",
    "# first three numbers of four-tuples should be one so kernel size and stride along batch, height, and width dimensions are 1\n",
    "# last value can be any kernels size and stride along the **depth** dimension and must be a be a divisor of the input depth (no. feature maps)\n",
    "output = tf.nn.max_pool(images, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_pool = keras.layers.Lambda(lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=\"VALID\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Average Pooling Layer\n",
    "- computes the mean of the entire feature map and outputs a single number. It's percetive field is the size of the feature map.\n",
    "    - outputs a single number per feature map and per instance\n",
    "- much information is lost, but it makes as a good output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"orange\">At this point the basic building blocks for CNNs have been covered</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
