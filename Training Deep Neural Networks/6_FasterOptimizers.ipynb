{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers\n",
    "- **IF NONE OF THESE WORK**: Use TensorFlow Model Optimization Toolkit (TF-MOT)\n",
    "## Momentum optimization\n",
    "- Good when the inputs are not normalized\n",
    "- inspiration: a bowling bowl rolling down a gentle slope. It starts out slow but eventually gets faster until terminal velocity\n",
    "- vs. regular Gradient Descent:\n",
    "    - regular GD takes small regular steps down the slope, so the algorithm takes more time to reach the bottom\n",
    "    - regular GD does not care about where the previous gradients were\n",
    "        - if the local gradient is tiny, it goes very slowly\n",
    "- Cares about the what the previous gradients were\n",
    "    - subtracts the local gradient from the *momentum vector* **m**, then updates the weights by adding this momentum vector\n",
    "        - the graient is used for acceleration, not for speed\n",
    "    - Beta hyperarameter, called \"momentum\", prevents the momentum from becoming too large (must be set betweewn 0 and 1)\n",
    "        - typical values is 0.9\n",
    "        \n",
    "\n",
    "<img src=\"images/MomentumEquation.jpeg\" width=360/>\n",
    "\n",
    "### verifying if the gradients remain constant\n",
    "- if the terminal velocity (the maximum size of the weight updates) is equal to that gradietn multiplied by the learning rate *eta* multiplied by 1/(1-Beta) ignoring the sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient (Nesterov momentum optimization)\n",
    "- measures the gradient of the cost function not at the loacl position theta but slightly ahead in the direction of the momentum, at *theta + Beta*m*\n",
    "- a variant to momentum optimization\n",
    "    - almost always faster than vanilla momentum opt.\n",
    "    \n",
    "<img src=\"images/NesterovMomentumEquation.jpeg\" width=360/>\n",
    "- **Why it works**\n",
    "    - the momentum vector will be pointing in the right direction (toward the optimum) in general\n",
    "       - more accurate to use the gradient measured a bit farther in the direction of the momentum vector than the gradient at the original position\n",
    "       - NAG is generally faster than regular momentum optimization    \n",
    "<img src=\"images/NAGGraph.jpeg\" width=360/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad \n",
    "- points the direction of descent closer to the global optimum\n",
    "- Preface:\n",
    "    - elongated bowl problem and Gradient Descent\n",
    "        - it points towards the steepest slope, not towards the global optimum\n",
    "- How it works:\n",
    "    - scales down the gradient vector along along the steepest dimensions\n",
    "- Short summary: \n",
    "    - \"Adaptive learning rate\" - it **decays the learning rate**, but does so faster for steep dimensions than for dimensions with gentler slopes\n",
    "        - helps point the resulting updates more direclty toward the global optimum. \n",
    "    - Requires much less tuning of the learning rate eta\n",
    "       \n",
    "- <img src=\"images/AdaGrad.jpeg\" width=360/>\n",
    "- Downsides\n",
    "    - can slow down too fast and never converge to the global optimum\n",
    "    - often stops too early when training neural networks because the learning rate gets scaled down too much before reaching the global optimum\n",
    "     - **Don't use for DNNs but can be used for Linear Regression for effiency**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "- Fixes AdaGrad's problem with not converging to the global optimum\n",
    "- Performs almost always better than AdaGrad\n",
    "- Was the most used optimiation algorithm until Adam optimization\n",
    "- How does it fix the problem?\n",
    "    - accumulates only the gradients from the most recent iterations (vs. all the gradients since the beginning of training)\n",
    "    - It does so by using exponential decay in the first step:\n",
    "    - <img src=\"images/RMSProp.jpeg\" width=360/>\n",
    "    - Decay rate Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9) # what the heck is rho?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam and Nadam Optimization\n",
    "- \"Adam\" - adaptive moment estimation\n",
    "- requires much less tuning of the learning rate beacuse it is an adaptive learning rate algorithm (like AdaGrad and RMSProp)\n",
    "- combines the ideas of momentum optimization and RMSProp\n",
    "    - similarities to momentum optimization - keeps track of an exponentially decaying average of past gradients\n",
    "    - similrities to RMSProp - keeps track of an exponentially decaying average of past squared gradients\n",
    "    - <img src=\"images/AdamOptimization.jpeg\" width=360/>\n",
    "- Variants:\n",
    "    - AdaMax\n",
    "    - Nadam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "- Adaptive optimization methods can lead to poorly generalizing networks\n",
    "- In this case **just use Nesterov Accelerated Gradient instead** because the dataset might just be incompatible to adaptive gradients\n",
    "\n",
    "### Extra Info:\n",
    "- all adaptive optimization techniques discussed so far only rely on *first-order partial derivatives (Jacobians)*\n",
    "- There are optimization algorithms based on *second-order partial derivatives (Hessians)*, which are the partial derivates of Jacobians\n",
    "    - Very hard to apply because n^2 Hessians per output (n==# of params) as opposed to n Jacobians per output\n",
    "        - can risk not fitting in memory and slowing down the backpropagation process too much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Models\n",
    "- Use if:\n",
    "    - blazingly fast runtime is needed\n",
    "    - have low memory\n",
    "    - prefer to use a sparse model instead (duh!)\n",
    "- How to get:\n",
    "    - Easy but not optimal way:\n",
    "        - train the model as usual\n",
    "        - get rid of the teeny tiny weights (set them to zero)\n",
    "    - Better way:\n",
    "        - apply strong l<sub>1</sub> regularization during training\n",
    "        - pushes the optimizer to zero out as many weights as it can\n",
    "- dangers:\n",
    "    - may degrade performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/OptimizerComparison.jpeg\" width=360/>\n",
    "* for bad, ** for average, *** for good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling\n",
    "- Considerably speeds up converage of learning rate\n",
    "- Very important to find a good learning rate\n",
    "    - if too high, training may diverge\n",
    "    - if too low, getting to global optimum will take a very long time\n",
    "    - if slightly too high, make progress very quick at first, but will end up dancing around the optimum, without ever converging\n",
    "    - if limited computing budget, training might be interrupted before converging, yielding a suboptimal solution\n",
    "    - <img src=\"images/LearningCurves.jpeg\" width=360/>\n",
    "- Try not to use constant learning rates\n",
    "- Use **Learning Schedules**\n",
    "    - Power scheduling\n",
    "    - Exponential scheduling\n",
    "    - Piecewise constant scheduling\n",
    "    - Performance scheduling\n",
    "    - 1cycle scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#power scheduling:\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4) # decay is the inverse of s (the # of steps it takes to divide the learning rate by one more unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate drops every ***s*** steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exponential scheduling w/ constant eta_0 (0.01) and s (20):\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch/20)\n",
    "\n",
    "#exponential decay w/ specifiable eta_0 and s:\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "expontial_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Rate Scheduler callback to be passed into the fit() method\n",
    "lr_scheduler_cb = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule function that relies on the optimizer's learning rate\n",
    "def exponential_decay_fun(epoch, lr):\n",
    "    return lr * 0.1**(1/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving a model, the learning rate and optimizer gets saved along with it, a trained model can continue training where it left off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, the epoch does **not** get saved and reset to 0 every time the *fit()* method is called and coudl lead to a very large learning rate and damage the model's weights\n",
    "    - solution: manually set the *intial_epoch* in the fit method so the schedule function can start at the right epoch every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piecewise constant scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance scheduling\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative to all above methods for learning rate scheduling, use **keras.optimizers.schedules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential_decay_fn() using optimizers.schedules\n",
    "\n",
    "x_train = [1, 2, 3] # dummy X_train\n",
    "s = 20 * len(x_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple and better way because learning rate and its schedule (plus the state) gets saved along with its model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
