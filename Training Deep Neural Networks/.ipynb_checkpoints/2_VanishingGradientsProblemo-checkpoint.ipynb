{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions:\n",
    "- vanishing gradients: gradients get smaller and smaller as the algorithm progresses down to the lower layers (closer to input layer); as a result, the gradient descent update leaves the lower layers' connection weights virtually unchanged, and training never converges to a good solution\n",
    "- exploding gradients: (generally surfaces in reurrent neural networks). It is when the gradients can grow bigger an bigger until layers get insanely large weight updates and the algorithm diverges\n",
    "\n",
    "\n",
    "## Preface:\n",
    "- The backpropagation algorithm works by going form the output layer to the input layer (foward feed then backward feed)\n",
    "- After computing the gradient of the cost function with regard to each parameter in the network, the agorithm\n",
    "- DNNs suffer from unstable gradients; different layres may learn at idely different speeds.\n",
    "- Vanishing/exploding gradients were the reasons why DNNs were mostly abandoned in the early 2000s\n",
    "\n",
    "\n",
    "## Problem: \n",
    "- Caused by a combination of logistic sigmoid activation function and the weight initialization technique that was most popular at the time (a gaussian distirbution with a mean of 0 and a std. dev of 1)\n",
    "    - this activation function and initialization scheme caused the variance of the outputs of each layer to be must greater than the variance of its inputs\n",
    "    - Going forward int he network, the variance (how far each number in the set is from the mean) keeps increasing after each laye runtil the activation function saturates at the top layers\n",
    "    - the logistic function has a mean of 0.5, not 0, which makes it perform slightly worse than hyperbolic tangent (tanh) activation function\n",
    "- In the logistic activation function when the inputs are large (negative or positive) the value is extremely close to 0 or 1 and the derivatives are extremely close to 0.\n",
    "    - With such a large variance at the top layers (the ranges are far away from the mean of 0.5), there would basically be no gradient to propagate back through the network's lowest layer.\n",
    "    - The small gradient that exists will keep getting diluted as backpropagation progresses down through the top layers, so nothing gets left for the lower layers (**thus vanishing**)\n",
    "    \n",
    "    \n",
    "## Solution:\n",
    "- Glorot and Bengio (the ones who discovered the cause of the problem) propose a way to alleviate the problem so that the signal flows properly in the forward direction when making prediction and properly backwards when backpropagating gradients\n",
    "    - Don't let the signal die out, nor make it explode or saturate\n",
    "- they propose that: \n",
    "    1.  the variance of the inputs and the outputs of each layer should be the same\n",
    "    2. gardients have equal variance before and after flowing through a layer in the reverse direction\n",
    "- It is **not actually possible** to do both unless # of inputs == # of neurons (*fan-in* == *fan-out*)\n",
    "- Compromise that works:\n",
    "    - **connection weights initialization: random where fav_avg=(fan_in+fan_out)/2**\n",
    "        - called the *Xavier initialization or Glorot initlization\n",
    "            - normal distribution with mean - and varianc sigma^2=1/fan_avg\n",
    "            - or uniform distribution between -r and +r, with r=sqrt(3/fan_avg)\n",
    "- different activation function have different initlizations and variance requirements (activation functions, initlizations, normal distrib variance):\n",
    "    - No activation, tanh, logistic (sigmoid), softmax -> Glorot init -> 1/fan_avg variance\n",
    "    - ReLU + variants -> He init -> 2/fan_in\n",
    "    - SELU -> LeCun init -> 1/fan_in\n",
    "- \"*He initialization*\" - the initialization strategy for the ReLU activation function and its variants\n",
    "    - the weights are initialized keeping in mind the size of the previous layer which helps in attaining a global minimum of the cost function faster and more efficiently\n",
    "    - The weights are still random but differ in range depending on the size of the previous layer of neurons\n",
    "    - This provides a controlled initialization hence the faster and more efficient gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras default: Gloot initialization + uniform distribution\n",
    "\n",
    "\n",
    "#### Changing initializations of a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "Dense(10, activation=\"reslu\", knernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonsaturating Activation Functions\n",
    "- Unstable gradients were caused partly by poor choise of activation function was discovered by Glorot adn Bengio\n",
    "    - ReLU activation function work well in DNN because it does not saturate for positive values\n",
    "\n",
    "### Problem with ReLU:\n",
    "- **dying ReLUs:**\n",
    "    - neurons lose the ability to output anything other than 0 during training\n",
    "        - can be caused by a large learning rate\n",
    "    - It dies when its weights are tweaked so that its weighted sum (input W*X) is negative for all instances in the training set\n",
    "        - *Does the bias make it 0?*\n",
    "        - Gradient descent won't afect it anymore because its gradient is zero when input is negative\n",
    "        \n",
    "### Solution for ReLU:\n",
    "- **Leaky ReLU**\n",
    "    - LeakyReLU_alpha(z)=max(alpha*z, z); *\"_alpha is a subscript\"*\n",
    "    - hyperparameter **alpha** defines how much the function \"leaks\"\n",
    "        - It is the slope of the function when z < 0\n",
    "        - typically set to 0.01\n",
    "        - Small slope ensures that leaky ReLUs **never die**, but they can go into a long coma with a chance to wakeup\n",
    "- Leaky variants of ReLU always outperformed strict ReLU activation function\n",
    "    - randomized leaky ReLU (RReLU)\n",
    "        - alpha is picked randomly in a given range during training and is fixed to an average value during testing\n",
    "    - parametric leaky ReLU (PReLU)\n",
    "        - alpha is allowed to be optimized during training so it can be modified by backpropagation\n",
    "- **Exponential Linear Unit (ELU)**\n",
    "    - outperformed all ReLU variants in the author's experiements\n",
    "        - reduced training time\n",
    "        NN perfomed better on the test set\n",
    "    - **ELU_alpha(z) = alpha(exp(z)-1) if z < 0 else z**\n",
    "    - **ELU vs ReLU:**\n",
    "        - takes on negative values when z < 0 which allows to mean output to be closer to 0 to help alleviate the vanishing gradients problem\n",
    "            - alpha (usually set to 1) defines the values that ELU approaches when z is a large negative number\n",
    "        - Nonzero gradient for z < 0 which avoids dead neurons\n",
    "        - if alpha = 1 (large negatives converge to -1) then the function is smooth everywhere including z=0 (there is an inflection in strict ReLU), which speeds up Gradient Descent sine it does not bounce much to the left and right (no abrupt change in slope?) of z=0\n",
    "    - **Drawbacks:**\n",
    "        - Slower to compute than ReLU and its variants b/c of exponential computations w/ the exponential function\n",
    "            - Faster converge rate *during training* compensates for it though\n",
    "        - Slower test time than ReLU\n",
    "- **Scaled ELU (SELU)**\n",
    "    - scaled variant of the ELU activation func.\n",
    "    - If a NN is built exclusively on a stack of dense (all neurons in prev layer connects to all neurons in current layer) layers, and if all hidden layers use the SELU activation function, the the network will **self-normalize**\n",
    "        - self-normalize: the output of each layer will tend to preserve a mean of 0 and an standard dev. of 1 during training, which solves the vanishing/exploding gradients problem\n",
    "    - Will often significantly outperform other ativation functions for NNs (especially DNNs)\n",
    "    - **Conditions for use:**\n",
    "        - Input features must be standardized (mean of 0 and standard dev of 1)\n",
    "        - Every hidden layer must be intialized with the LeCun normal initialization\n",
    "        - The NN must have sequential architecture, so it excludes RNNs and networks with skip connections (e.g. Wide & Deep nets). Otherwise, SELU will not self-normalize\n",
    "        - SELU can help CNNs as well.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So which activation function to choose?\n",
    "- **SELU > ELU > leak ReLU (and variants) > ReLU > tanh > logistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
