{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization (BN)\n",
    "- preface:\n",
    "    - He initialization w/ ELU (or other ReLU variants) can significantly reduce the risk of vanishing/exploding gradients problems at the **beginning** of training, it does not gaurantee that the problems will arise **during training**\n",
    "- Batch normalization is a technique where an operation is added just before or after the activation function of each hidden layer.\n",
    "    - Simply, it zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer, one for scaling and one for shifting.\n",
    "    - In even simpler terms, it lets the model learn the optimal scale and mean of each of the layer's inputs\n",
    "- If BN is added at the very beginning of the model, the training set does not need to be standardized\n",
    "    - BN will handle standardazation one batch at a time\n",
    "- Positive Effects:\n",
    "    - huge improvement in the ImageNet classification task\n",
    "    - vanishing gradients problem was strongly reduced, to the point that they could use saturating activation functions like tanh and logisitic sigmoid activation function\n",
    "    - NNs are much less sensitive to weight initialization\n",
    "    - much larger learning rates can be used to speed up learning process\n",
    "    - Acts like a regularizer to reduce the need for regularization techniques\n",
    "- Negative Effects:\n",
    "    - adds some complexity to the model\n",
    "    - runtime penalty: NN makes slower predictions due to the extra computations required at each layer\n",
    "        - remedy: to combine the BN layer w/ the previous layer after training by updating the weights and biases of the previous layer w/ that computed by the BM layer. Thus the BN layer can be rid off\n",
    "\n",
    "\n",
    "### BN steps\n",
    "<img src=\"images/BatchNormalizationEquation.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/BNExplanation.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In short...\n",
    "- During training, BN standardizes its inputs, then rescales and offsets them.\n",
    "\n",
    "#### BN when testing\n",
    "- Problems:\n",
    "    - if only one instance is fed in at a time\n",
    "        - BN can't get a mean and standard deviation\n",
    "    - if a batch is fed in\n",
    "        - Could be too small\n",
    "        - or may not be independent and identically distributed (IID)\n",
    "- Solution:\n",
    "    - train the NN then run the whole training set through the NN again and compute the mean and standard deviation of each input of the BN layer\n",
    "    - the input means and standard deviations computed will be used as the input means and std. when making predictions\n",
    "    - note: most implementations compute the moving average of the means and std.\n",
    "        - Keras does this too\n",
    "    - gamma, Beta, mu, and sigma are learned in each batch-normalized layer\n",
    "        - mu (final input mean vector) and sigma (final input std. vector) are not used during training but used for batch inputs for testing\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlikely that BN will have a significant impact on such a **shallow** network. It is **much for effective** on **deeper** networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors of the BN paper debated about if the BN layers sohuld be before or after the activation function (above).\n",
    "- Experiment which one works better with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
