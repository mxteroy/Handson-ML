{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoid Overfitting Through Regularization\n",
    "- \"With four parameters I can fit an elephant and with fie I can make him wiggle his trunk\" by John von Neumann\n",
    "- preface: DNNs can have **tens of thousands or even millions** of parameters\n",
    "    - increases flexibility and can fit a huge variety of complex datasets\n",
    "    - flexibility can make the network prone to overfitting\n",
    "- Important techniques covered in previous chapters:\n",
    "    - early stopping\n",
    "    - Batch normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l<sub>1</sub> and l<sub>2</sub> Regularization\n",
    "- l<sub>1</sub> for a sparse model (with many weights equal to 0)\n",
    "- l<sub>2</sub> to constrain a NN's connection weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", \n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The l<sub>2</sub>() function returns a regularizers that's called at each step during training to compute regularization loss\n",
    "    - then it is added to the final loss \n",
    "- Typically, I want to apply the same regularizer to all layers in my network, and use the same activation func and intialization strategy in all hidden layers\n",
    "    - Use *functools.partial()* function to avoid code repititions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "#history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one of the most popular regular. techniques\n",
    "- Even state-of-the-art NNs get 1-2% accuracy boosts\n",
    "- At every training step:\n",
    "    - every neuron has a probability *p* of being \"dropped out,\" meaning that it will be ignored during this training step\n",
    "        - but it could be active the next step\n",
    "        - hyperparameter *p* == dropout rate; typically b/t 10%-50%, 20%-30% in Rnns, 40%-50% in Cnns\n",
    "        - <img src=\"images/Dropout.jpeg\" width=360/>\n",
    "- neurons cannot co-adapt w/ neighboring neurons, they have to be as useful as possible on their own\n",
    "- neurons cannot rely exclusively on just a few input neurons; they must pay attention to all of their input neurons\n",
    "- result: less sensitivity to slight changes in input, meaning more robust network!\n",
    "- **alternative way to interpret Dropout**\n",
    "    - it's lke a unique NN is generated at each trainign step\n",
    "    - a total of 2<sup>N</sup> possible networks because each neuron can be either present or absent\n",
    "    - The whole network can be seen as an averaging ensemble of all these smaller NNs\n",
    "- It will need to multiple each connection weight w/ the *keep probability (1-p)* so that each neuron won't get a total input signal roughly as large as what the network was trained on \n",
    "- During training, the dropout layer randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability\n",
    "- After training, the dropout layer does nothing at all. Just passes the inputs to the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "#history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Warning: dropout is only active during training, so training loss should not be compared to validation loss. Evaluate w/out dropout\n",
    "- If overfitting, increase dropout rate\n",
    "- If underfitting, decrease dropout rate\n",
    "- Increase dropout rate for large layers, and decrease for small ones\n",
    "- significantly slows convergence but performs better generally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) Dropout\n",
    "- A paper further justified dropout\n",
    "    1. relationship b/t dropout networks and approximate Bayesian inference found\n",
    "    2. MC dropout was introduced to boost the performance of any trained ropout model without having to retrain it or even modify it at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
