{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yann LeCun: \"if intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake\"\n",
    "\n",
    "The most common unsupervised learning task: **dimensionality reduction**\n",
    "\n",
    "Other types of unsurpervised learnin tasks and algorithms:\n",
    "- Clustering\n",
    "    - goal: group similar instances into \"clusters\"\n",
    "    - Uses: data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.\n",
    "- Anomaly detection\n",
    "    - goal: learn what \"normal\" data looks like, and then use that to detect abnormal instances (anomalies) \n",
    "    - uses: detecting defective items on a production line or a new trend on a time series.\n",
    "- Density estimation\n",
    "    - goal: detect the probability density function of the **random process** that generated the dataset.\n",
    "    - uses: most commonly used in anomaly detection (instances in low density regions are anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Example situation\n",
    "- Finding different but similar plants in the forest, but not knowing what species they are. I can group (cluster) them together because they are similar\n",
    "\n",
    "Clustering is like classification except it is unsurpervised (no labels)\n",
    "It makes good use of all features\n",
    "\n",
    "Applications:\n",
    "- Customer Segmentation\n",
    "    - Cluster customers based on their purchases and activity on a website\n",
    "    - Useful to understand who the customers are and what they need\n",
    "    - Recommender systems can be built. Recommend something that others enjoyed in the same cluster to a user\n",
    "- Data Analysis\n",
    "    - Run a clsutering algorithm on a dataset and analyze each cluster\n",
    "- Dimensionality Reduction\n",
    "    - Measure the **affinity (the measure of how well an instance fits into a cluster)** of an instance to each cluster in the dataset. Replace the instances feature vector w/ its affinity vector of length k, which is normally much lower-dimensionality than the original feature vector without losing much information. Overall, the dataset is reduced to k dimensions.\n",
    "- Anomaly Detection\n",
    "    - Detect unusual behavior of customers (fraud detection)\n",
    "- Semi-supervised Learning\n",
    "    - If only a few labels are available, propagate the labels to all instances in the same cluster. Run a supervised learning algorithm on the fully-labeled dataset\n",
    "- Search Engines\n",
    "    - Searching for similar images\n",
    "        - run a clustering algo on all images in database\n",
    "- Image Segmentation\n",
    "    - Cluter pixels according to color and replace each pixel with the mean color of its cluster to reduce the number of different colors in image\n",
    "    - Makes it easier to detect the contour of each object\n",
    "    - Used in object detection and tracking systems\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "## K-Means\n",
    "\n",
    "Simple algorithm that clusters datasets with clear blobs (clusters) very quickly and efficiently.\n",
    "Proposed by Stuart Lloyd at Bell Labs in 1957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 5 # no. of clusters. It is helpful to plot the dataset prior\n",
    "kmeans = KMeans(n_clusters=k) # each predicted label is the index of cluster that it applies to. so it is in positive whole number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It does not behave well when blobs' diameters are very different from each other because it assigns an instance's cluster based on its distance to the centroid (center of a cluster)\n",
    " \n",
    " Types of clustering:\n",
    " - hard clustering: only assigning an instance to one cluster\n",
    " - soft clustering: calculates the scores (distance to centroid of similarity/affinity score) of an instance to each cluster\n",
    "     - **can be very efficient to reduce dimensions**\n",
    "     \n",
    "     \n",
    "     \n",
    "#### K-means algorithm:\n",
    "- pick random centroids -> label instances -> repeat until the algorithm converges\n",
    "- Time complexity: \n",
    "    - data has clustering structure: linear with regard to m, k, n; m = # of instances, k = # of clusters, n = # of dimensions\n",
    "    - data doesn't have clustering strucutre: increase exponentially w/ m. *rarely happens*\n",
    "\n",
    "**note: it is not guaranteed to converge to the optimal clusters**\n",
    "\n",
    "To increase chance of reaching optimal clustering:\n",
    "##### Centroid initlialization methods:\n",
    "- if approx. centroid are known (from running previous clustering algo; from visualization) initialize the centroids in KMeans object in Scikit\n",
    "- Run K-Means multiple times and keep the best solution. Controlled by n_init hyperparameter (default=10)\n",
    "\n",
    "How is \"best solution\" measured between K-Means models?\n",
    "- Metric **inertia**: the means squared distance between each instance and its closest centroid\n",
    "- Model with lowest inertia is kept\n",
    "- *a model's score is the negative intertia in Scikit because of Scikit's rule \"greater is better\"*\n",
    "\n",
    "#### K-means++ (improves K-means initialization):\n",
    "Basically pick initialization centroids that are the furthest distances from one another. The default of KMeans in Scikit\n",
    "\n",
    "Other improvements for K-means:\n",
    "- Accelerated K-means\n",
    "    - avoids unnecessary distance calculations by exploiting the \"a straight line is always the shortest distance between two points\" rule\n",
    "    - default algorithm in Scikit\n",
    "- Mini-batch K-means:\n",
    "    - Instead of using the full dataset, use mini-batches to move the centroids slightly per iteration\n",
    "    - Speeds up algorithm by 3x-4x\n",
    "    - Makes it possible to clusters huge datasets that won't fit in memory\n",
    "    - Implemented in Scikit as MiniBatchKMeans\n",
    "    - downside: inertia is much worse\n",
    "    \n",
    "What if the dataset does not fit in memory (not just for K-means)?\n",
    "- use memmap class in NumPy\n",
    "or\n",
    "- pass one mini-batch at a time to *partial_fit()* in MiniBatchKMeans\n",
    "    - much more tedious method. do not use\n",
    "    \n",
    "    \n",
    "### Methods for choosing the right number of clusters k:\n",
    "*intertia will not work because the current number of clusters could be higher than the optimal number (lower inertia). The more clusters the shorter the distances are from an instance to its nearest centroid.*\n",
    "1. Manual way: plot inertia vs. k and find the \"elbow.\" use the k at that elbow\n",
    "2. More precise way: **silhouette score** (b-a)/max(a,b). Plot the silhouette score vs. k. K with higher score is better\n",
    "    - a is the mean distance to the other instance in the same cluster\n",
    "    - b is the mean distance to the instances of the next closest cluster (one that maximizes b, excluding instances of one's own cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limits of clustering\n",
    "- does not perform well on elliptical clusters. Rather, use Gaussian mixture models\n",
    "\n",
    "### Make K-means perform better\n",
    "- scale the input features so the chances of spherical clusters is greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
