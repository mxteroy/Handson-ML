{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model Exercises pg. 145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>converge - tends to a limit</i>\n",
    "    \n",
    "<i>regularize a model - to constrain a model by reducing its degrees of freedom and decrease variance</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If there are millions of features in a linear regression problem, the best algorithm to use would be a gradient descent algorithm (stochastic, mini-batch, and batch if there is enough memory). Using the normal equation would be too slow because it scales poorly with the number of features in the dataset. Alternatively, the gradient descent scales poorly with the number of training instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature scaling is helpful when the dataset has very different scales for features. The algorithm that suffers from this is gradient descent because it will resemble an elongated bowl (pg 115) when the features have different scales. The downside to this is that reaching the global minimum for the cost function will take longer and take a rougher path (elongated bowl shape). As a solution, scale the data before training! Smaller values are deemed as less important (thus ignored) when features have different scales.\n",
    "\n",
    "    **The normal equation does not need feature scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. No because the cost function will be convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Depends. If the model is linear or logistic, the cost function is convex, and the learning rate isn't too high, then all the GD algorithms will approach the global minimum and results in similar models. Although, the learning rate must be gradually decreased because otherwise stochastic and mini-batch GD will never fully converge to the global minimum. Even if they are run for a very long time, they will produce slightly different models unless the learning rate is adjusted while converging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Validation error increasing means the learning rate is either too high and the GD algorithm is diverging. If the training [set] error is also increasing then it is a clear sign that the learning rate is too **high**. If the training set error is not increasing, then the problem is **overfitting** and that training should be halted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. No, because mini-batch GD is a erratic and might still be converging to the global minimum, and it might also be escaping a local minimum. Rather, save the model at each training iteration, and stop training if the training error is not getting better then revert to the best saved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Stochastic (since it considers one training instance at a time) and mini-batch w/ small mini-batch sizes reach the vicinity of the global min the fastest, but they won't actually converge to the global min (unless learning rate is gradually decreased with a learning schedule) but, rather, get very close to it. Batch actually does converge given enough training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Overfitting or underfitting. To solve:\n",
    "    1. overfitting - decrease the polynomial order (polynomial degree). underfitting - increase the polynomial order (polynomial degree)\n",
    "    2. regularize the model by using a penalized cost function (ridge or lasso)\n",
    "    3. get more data! increase the size of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. <i>high bias - higher error on training and test data (underfitting). high variance - low error on training data but high error on test data (overfitting) </i>.\n",
    "\n",
    "    The model suffers from high bias because it is underfitting both the training and validation sets. Thus, the regulization hyperparameter, alpha, should be decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. When to use:\n",
    "    1. Linear regression vs. ridge regression:\n",
    "        - regularized models typically perform better than plain lin. reg.\n",
    "    2. Lasso regression vs. ridge regression:\n",
    "        - Lasso regression sets the weights of the \"unimportant\" feature to zero (automatic feature scaling)\n",
    "        - Choose Lasso when it is known that only a few features matter\n",
    "        - Choose ridge when it is not known\n",
    "    3. Lasso regression vs. Elastic net regression\n",
    "        - Use elastic net (mix of ridge and lasso) because lasso behaves erratically when the # of features is greater than # of instances or when most of the features are strongly correlated\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Batch GD with early stopping for Softmax regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "data = load_iris()\n",
    "X = data[\"data\"][:, (2, 3)] #only petal length and width\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and test sets from scratch\n",
    "\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(test_ratio * total_size)\n",
    "validation_size = int(validation_ratio * total_size)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train, y_train = X_with_bias[rnd_indices[:train_size]], y[rnd_indices[:train_size]]\n",
    "X_validation, y_validation = X_with_bias[rnd_indices[train_size:-test_size]], y[rnd_indices[train_size:-test_size]] # index of -test_size is shorthand of len(X_width_bias) - test_size\n",
    "X_test, y_test = X_with_bias[rnd_indices[-test_size:]], y[rnd_indices[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax uses classified probabilities \n",
    "\n",
    "def one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    y_one_hot = np.zeros((len(y), n_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1 \n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = one_hot(y_train)\n",
    "y_validation_one_hot = one_hot(y_validation)\n",
    "y_test_one_hot = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(softmax_scores):\n",
    "    exps = np.exp(softmax_scores)\n",
    "    return exps / np.sum(exps,axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.446205811872683\n",
      "500 0.8350062641405651\n",
      "1000 0.6878801447192402\n",
      "1500 0.6012379137693314\n",
      "2000 0.5444496861981872\n",
      "2500 0.5038530181431525\n",
      "3000 0.47292289721922487\n",
      "3500 0.44824244188957774\n",
      "4000 0.4278651093928793\n",
      "4500 0.41060071429187134\n",
      "5000 0.3956780375390374\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01 #define learning rate\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7 #add to Pk^i in log(Pk^i) because to prevent log(0) error\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs) #random initializations for feature weights. shape = n_features x n_classes. Each class has their own dedicated parameter vector\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    softmax_scores = X_train.dot(Theta)\n",
    "    y_proba = softmax(softmax_scores)\n",
    "    cost = -np.mean(np.sum(y_train_one_hot*np.log(y_proba + epsilon), axis=1))\n",
    "    error = y_proba - y_train_one_hot\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, cost)\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.32094157, -0.6501102 , -2.99979416],\n",
       "       [-1.1718465 ,  0.11706172,  0.10507543],\n",
       "       [-0.70224261, -0.09527802,  1.4786383 ]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_score_valid = X_validation.dot(Theta)\n",
    "Y_proba = softmax(softmax_score_valid)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.31466679236949\n",
      "500 0.5378947790755032\n",
      "1000 0.5046573947978396\n",
      "1500 0.49505389686784407\n",
      "2000 0.4914430657465934\n",
      "2500 0.4899610932392464\n",
      "3000 0.48932601099063844\n",
      "3500 0.48904708136462205\n",
      "4000 0.4889227267817381\n",
      "4500 0.4888667580914424\n",
      "5000 0.48884141287504024\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "eta = 0.1 #define learning rate\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7 #add to Pk^i in log(Pk^i) because to prevent log(0) error\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs) #random initializations for feature weights. shape = n_features x n_classes. Each class has their own dedicated parameter vector\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    softmax_scores = X_train.dot(Theta)\n",
    "    y_proba = softmax(softmax_scores)\n",
    "    l2_loss = (alpha/2) * np.sum(np.square(Theta[1:])) #skip the first element\n",
    "    cost = -np.mean(np.sum(y_train_one_hot*np.log(y_proba + epsilon), axis=1)) + l2_loss\n",
    "    error = y_proba - y_train_one_hot\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, cost)\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_score_valid = X_validation.dot(Theta)\n",
    "Y_proba = softmax(softmax_score_valid)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4810766085956093\n",
      "500 0.5273833023235621\n",
      "1000 0.5017773436353987\n",
      "1500 0.4940082947584887\n",
      "2000 0.4910219013615579\n",
      "2500 0.4897825069610757\n",
      "3000 0.48924807180697094\n",
      "3500 0.4890124718111627\n",
      "4000 0.4889071897601139\n",
      "4500 0.4888597339906442\n",
      "5000 0.48883822270267346\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "eta = 0.1 #define learning rate\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7 #add to Pk^i in log(Pk^i) because to prevent log(0) error\n",
    "best_loss = np.infty\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs) #random initializations for feature weights. shape = n_features x n_classes. Each class has their own dedicated parameter vector\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    softmax_scores = X_train.dot(Theta)\n",
    "    y_proba = softmax(softmax_scores)\n",
    "    l2_loss = (alpha/2) * np.sum(np.square(Theta[1:])) #skip the first element\n",
    "    cost = -np.mean(np.sum(y_train_one_hot*np.log(y_proba + epsilon), axis=1)) + l2_loss\n",
    "    error = y_proba - y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients\n",
    "    \n",
    "    softmax_scores = X_train.dot(Theta)\n",
    "    y_proba = softmax(softmax_scores)\n",
    "    l2_loss = (alpha/2) * np.sum(np.square(Theta[1:])) #skip the first element\n",
    "    cost = -np.mean(np.sum(y_train_one_hot*np.log(y_proba + epsilon), axis=1)) + l2_loss\n",
    "    error = y_proba - y_train_one_hot\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, cost)\n",
    "    if cost < best_loss:\n",
    "        best_loss = cost\n",
    "    else:\n",
    "        print(iteration-1, best_loss)\n",
    "        print(iteration, loss, 'loss increased! early stopping!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_score_valid = X_validation.dot(Theta)\n",
    "Y_proba = softmax(softmax_score_valid)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_validation)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_score_valid = X_test.dot(Theta)\n",
    "Y_proba = softmax(softmax_score_valid)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
