{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If all the models are different, then they can be combined for better results with a voting ensemble to yield an better model. It works better if the models are different or if the models have some elements of randomness.\n",
    "\n",
    "2. Hard voting classifiers have a majority vote system in which the classes that are \"voted\" the most are predicted. Soft voting classifiers compute and then average out all the probabilities for each class and predicts the class that has the highest average probability. Soft voting only works with classifiers that can output class probabilities\n",
    "\n",
    "3. Bagging can be sped up and distributed among multiple servers. Each server could contain its own subset of the data and a classifier. \n",
    "    - The same thing can be done with pasting except without replacement\n",
    "    - boosting ensembles don't need it because it trains sequentially. Two models don't train at the same time\n",
    "    - Different serves could train different trees of a random forest\n",
    "    - stacking ensembles can be sped up by having different models on different servers and he blender (meta learner) on its own server. redictors on one layer can only be trained by the predictors in the layer before it\n",
    "   \n",
    "4. Out of bag (OOB) instances are ones that have not been seen by the predictors in a bagging ensemble when it trains predicts on random subsets. On average, only 63% of instances in a training set are seen, while the other 37% are not. With OOB evaluation, the OOB instances are used to evalute the model that was trained. *oob_score param in BaggingClassifier for Scikit*\n",
    "\n",
    "5. Extra trees randomizes the threshold for the splitting of the features rather than using the best possible threshold that regular decision trees do. This method is faster a regular random forest because finding the best possible threshold for splitting is the most time consuming task of decision trees. \n",
    "\n",
    "6. If the adaboost ensemble is underfitting the training data, the hyperparameters to tweak is the number of estimators. Increase the number of estimators. Also, find the best hyperparameters for the base estimator.\n",
    "\n",
    "7. In Gradient Boosting, *learning_rate* scales the weight of each tree. So a higher learning_rate will decrease the number of predictors needed to fit the training data, but will have low generalization and increase the chance of overfitting; it will also run faster. Vice versa for lower learning_rate\n",
    "\n",
    "## 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X_train, X_residue, y_train, y_residue = train_test_split(X, y)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_residue, y_residue, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(probability=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9406528189910979"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)\n",
    "rf_clf.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9643916913946587"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_clf.fit(X_train, y_train)\n",
    "et_clf.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9851632047477745"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)\n",
    "svm_clf.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = [('random forest', rf_clf), ('extra trees', et_clf), ('SVM classifier', svm_clf)]\n",
    "\n",
    "hard_voting_clf = VotingClassifier(estimators, voting=\"hard\")\n",
    "soft_voting_clf = VotingClassifier(estimators, voting=\"soft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9851632047477745"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_voting_clf.fit(X_train, y_train)\n",
    "hard_voting_clf.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9910979228486647"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_voting_clf.fit(X_train, y_train)\n",
    "soft_voting_clf.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911504424778761"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911504424778761"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911504424778761"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {\"data\": np.empty((len(y_validation), len(estimators)), dtype=np.float32), \"target\": y_validation}\n",
    "\n",
    "for i, (_, estimator) in enumerate(estimators):\n",
    "    y_validation_pred = estimator.predict(X_validation)\n",
    "    data[\"data\"][:, i] = y_validation_pred #each feature are the predictions of one estimator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_blender = RandomForestClassifier(oob_score=True, random_state=42)\n",
    "rf_clf_blender.fit(data[\"data\"], data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9792284866468842"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_blender.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\"data\": np.empty((len(y_test), len(estimators)), dtype=np.float32), \"target\": y_test}\n",
    "\n",
    "for i, (_, estimator) in enumerate(estimators):\n",
    "    test_data[\"data\"][:, i] = estimator.predict(X_test)\n",
    "    \n",
    "y_pred = rf_clf_blender.predict(test_data[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823008849557522"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is lower than the voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
