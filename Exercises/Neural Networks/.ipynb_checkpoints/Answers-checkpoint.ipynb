{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A xor b = (A or B) and (not A or not B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regular Perceptron does not outputs probabilities, rather it outputs hard values. Perceptron is also unable to solve complex problems because it is a linear algorithm. It can only converge when the dataset is linearly separable. To make perceptron equivalent to a Logistic Regression classifier, it must be have a logisitc activation function (or softmax if there are multiple neurons) and trained with Gradient Descent to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The logistic (sigmoid) activation function was key to the first MLPs because it improves the performance of Gradient Descent due to being continuous, while the classic activation function was a step function (had derivatives of 0) and did not enable Gradient Descent to \"descend\" because there is no slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The three popular activation functions:\n",
    "    - Sigmoid (logistic)\n",
    "    - Hypertangent\n",
    "    - Rectified Linear Unit (ReLU)\n",
    "    - Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. \n",
    "    - The shape of the input matrix is N * 10 (N instances; 10 features).\n",
    "    - The shape of the hidden layers weight vector is 50x1. The bias vector has the same shape\n",
    "    - The shape of the output layer's weight vector is 3x1. The bias vector has the same shape\n",
    "    - The shape of the network's output matrix Y is 3x1. One for each output neuron (class)\n",
    "    - ~Y = max(Wh*max(W0*X+B0)+Bh, 0)~ \n",
    "        - Textbook answer: Y = max(W0*max(Wh*X + Bh) + B0, 0)\n",
    "            - it is a chain of functions, with the complexity increasing at every layer. The chaining begins from the first hidden layer's activation function to the output layer's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. To classify email into smap or ham, I need one output neuron. The activation function should be a logistic function (to normalize the output between 0 and 1 and use it as a probability). For MNIST, I would need 10 neurons in the output layer, and the activation function should be the softmax activation function to handle one probability per class. To predict housing prices, there would need to be one output neuron, and no output activation function to outputa specific value (this is done in regression problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. - Backpropagation works by (in order):\n",
    "    1. feeding the input into the network as batches\n",
    "    2. computing the outputs of each layer until the output layer\n",
    "    3. computes the loss of the network by comparing its output with the actual/desired output\n",
    "    4. goes through the network in reverse and calculates how much each output (neuron) connection contributed to the error by using the *chain rule*\n",
    "    5. then measures how much each connection in the layer below contributed to the error by using the chain rule, and repeats until the input layer\n",
    "        - it works by propagating the error gradient backward through the network to compute the error gradient across all connection weights\n",
    "    6. then it does a gradient descent step to change the connection weights by using the error gradients it computed\n",
    " - Reverse-mode autodiff is only a part of backprogation and it used to automatically differentiate the (activation?) functions\n",
    "   \n",
    "books answer:\n",
    "- Backprop is a technique to train ANNs. It computes the gradients of the cost function with regard to every model parameter (all weights and biases), then performs a gradient descent step using these gradients. This step is usually perfomed thousands or millions of times, using many training batches, until the model parameters converge to the values that minimize the cost function. \n",
    "- Backprop uses reverse-mode autodiff to compute the gradients. Reverse-mode autodiff performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once.\n",
    "- Reverse-mode autodiff is just a part of backpropagation that is used to compute gradients efficiently (the process of training the whole NN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. - Hyperparameters that can be tweak in a basic MLP\n",
    "    - \\# of hidden layers\n",
    "    - \\# of neurons per layer\n",
    "    - learning rate\n",
    "    - activation function\n",
    "    - epochs\n",
    "    - batch size\n",
    "    - \\# of operations\n",
    "- To combat overfitting, use early stopping and use other regularization techniques, reduce the number of hidden layers, and reduce the number of neurons per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25)\n",
    "x_test =  x_test\n",
    "y_test =  y_test\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.true_divide(x_train, 255.0)\n",
    "x_val = np.true_divide(x_val, 255.0)\n",
    "x_test = np.true_divide(x_test, 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=x_train.shape[1:]),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    Dense(50, activation=\"relu\"),\n",
    "    Dense(10, activation=\"softmax\") # multiclass classification problem. mutually exclusive classes\n",
    "])\n",
    "\n",
    "alpha = 1e-3\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=SGD(learning_rate=alpha), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 2.2858 - accuracy: 0.1544 - val_loss: 2.2005 - val_accuracy: 0.2677\n",
      "Epoch 2/30\n",
      "1407/1407 [==============================] - 1s 832us/step - loss: 2.1382 - accuracy: 0.3044 - val_loss: 1.8336 - val_accuracy: 0.5273\n",
      "Epoch 3/30\n",
      "1407/1407 [==============================] - 1s 842us/step - loss: 1.6897 - accuracy: 0.5824 - val_loss: 1.2060 - val_accuracy: 0.6790\n",
      "Epoch 4/30\n",
      "1407/1407 [==============================] - 1s 850us/step - loss: 1.0733 - accuracy: 0.7053 - val_loss: 0.8010 - val_accuracy: 0.7556\n",
      "Epoch 5/30\n",
      "1407/1407 [==============================] - 1s 860us/step - loss: 0.7500 - accuracy: 0.7766 - val_loss: 0.6412 - val_accuracy: 0.8073\n",
      "Epoch 6/30\n",
      "1407/1407 [==============================] - 1s 842us/step - loss: 0.6090 - accuracy: 0.8191 - val_loss: 0.5470 - val_accuracy: 0.8384\n",
      "Epoch 7/30\n",
      "1407/1407 [==============================] - 1s 836us/step - loss: 0.5373 - accuracy: 0.8443 - val_loss: 0.4798 - val_accuracy: 0.8605\n",
      "Epoch 8/30\n",
      "1407/1407 [==============================] - 1s 831us/step - loss: 0.4644 - accuracy: 0.8664 - val_loss: 0.4296 - val_accuracy: 0.8755\n",
      "Epoch 9/30\n",
      "1407/1407 [==============================] - 1s 834us/step - loss: 0.4093 - accuracy: 0.8845 - val_loss: 0.3935 - val_accuracy: 0.8870\n",
      "Epoch 10/30\n",
      "1407/1407 [==============================] - 1s 827us/step - loss: 0.3804 - accuracy: 0.8930 - val_loss: 0.3623 - val_accuracy: 0.8969\n",
      "Epoch 11/30\n",
      "1407/1407 [==============================] - 1s 839us/step - loss: 0.3495 - accuracy: 0.8999 - val_loss: 0.3492 - val_accuracy: 0.8987\n",
      "Epoch 12/30\n",
      "1407/1407 [==============================] - 1s 827us/step - loss: 0.3273 - accuracy: 0.9063 - val_loss: 0.3273 - val_accuracy: 0.9059\n",
      "Epoch 13/30\n",
      "1407/1407 [==============================] - 1s 832us/step - loss: 0.3149 - accuracy: 0.9089 - val_loss: 0.3208 - val_accuracy: 0.9076\n",
      "Epoch 14/30\n",
      "1407/1407 [==============================] - 1s 843us/step - loss: 0.3036 - accuracy: 0.9139 - val_loss: 0.3011 - val_accuracy: 0.9136\n",
      "Epoch 15/30\n",
      "1407/1407 [==============================] - 1s 844us/step - loss: 0.2881 - accuracy: 0.9176 - val_loss: 0.2905 - val_accuracy: 0.9164\n",
      "Epoch 16/30\n",
      "1407/1407 [==============================] - 1s 832us/step - loss: 0.2832 - accuracy: 0.9183 - val_loss: 0.2801 - val_accuracy: 0.9199\n",
      "Epoch 17/30\n",
      "1407/1407 [==============================] - 1s 846us/step - loss: 0.2597 - accuracy: 0.9250 - val_loss: 0.2742 - val_accuracy: 0.9195\n",
      "Epoch 18/30\n",
      "1407/1407 [==============================] - 1s 837us/step - loss: 0.2598 - accuracy: 0.9243 - val_loss: 0.2624 - val_accuracy: 0.9243\n",
      "Epoch 19/30\n",
      "1407/1407 [==============================] - 1s 838us/step - loss: 0.2486 - accuracy: 0.9292 - val_loss: 0.2578 - val_accuracy: 0.9253\n",
      "Epoch 20/30\n",
      "1407/1407 [==============================] - 1s 838us/step - loss: 0.2369 - accuracy: 0.9306 - val_loss: 0.2499 - val_accuracy: 0.9287\n",
      "Epoch 21/30\n",
      "1407/1407 [==============================] - 1s 834us/step - loss: 0.2320 - accuracy: 0.9316 - val_loss: 0.2450 - val_accuracy: 0.9300\n",
      "Epoch 22/30\n",
      "1407/1407 [==============================] - 1s 959us/step - loss: 0.2241 - accuracy: 0.9341 - val_loss: 0.2374 - val_accuracy: 0.9316\n",
      "Epoch 23/30\n",
      "1407/1407 [==============================] - 1s 887us/step - loss: 0.2204 - accuracy: 0.9353 - val_loss: 0.2363 - val_accuracy: 0.9319\n",
      "Epoch 24/30\n",
      "1407/1407 [==============================] - 1s 842us/step - loss: 0.2190 - accuracy: 0.9363 - val_loss: 0.2381 - val_accuracy: 0.9322\n",
      "Epoch 25/30\n",
      "1407/1407 [==============================] - 1s 847us/step - loss: 0.2121 - accuracy: 0.9381 - val_loss: 0.2225 - val_accuracy: 0.9357\n",
      "Epoch 26/30\n",
      "1407/1407 [==============================] - 1s 847us/step - loss: 0.2060 - accuracy: 0.9402 - val_loss: 0.2275 - val_accuracy: 0.9341\n",
      "Epoch 27/30\n",
      "1407/1407 [==============================] - 1s 842us/step - loss: 0.1966 - accuracy: 0.9428 - val_loss: 0.2173 - val_accuracy: 0.9372\n",
      "Epoch 28/30\n",
      "1407/1407 [==============================] - 1s 840us/step - loss: 0.1961 - accuracy: 0.9439 - val_loss: 0.2104 - val_accuracy: 0.9384\n",
      "Epoch 29/30\n",
      "1407/1407 [==============================] - 1s 840us/step - loss: 0.1944 - accuracy: 0.9433 - val_loss: 0.2076 - val_accuracy: 0.9399\n",
      "Epoch 30/30\n",
      "1407/1407 [==============================] - 1s 846us/step - loss: 0.1923 - accuracy: 0.9441 - val_loss: 0.2124 - val_accuracy: 0.9380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf781ffeb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=30, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameseroy/ml/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict_classes(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_neurons=10, n_hidden=1, learning_rate=1e-3, input_shape=x_train.shape[1:]):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=SGD(learning_rate=learning_rate), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "mnist_clf = KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "704/704 [==============================] - 1s 880us/step - loss: 0.4177 - accuracy: 0.8749\n",
      "[CV] END ...........................n_hidden=7, n_neurons=78; total time=  15.3s\n",
      "704/704 [==============================] - 1s 863us/step - loss: 0.4189 - accuracy: 0.8711\n",
      "[CV] END ...........................n_hidden=7, n_neurons=78; total time=  15.1s\n",
      "704/704 [==============================] - 1s 897us/step - loss: 0.3422 - accuracy: 0.9001\n",
      "[CV] END ...........................n_hidden=6, n_neurons=99; total time=  16.4s\n",
      "704/704 [==============================] - 1s 1ms/step - loss: 0.3874 - accuracy: 0.8874\n",
      "[CV] END ...........................n_hidden=6, n_neurons=99; total time=  16.1s\n",
      "704/704 [==============================] - 1s 819us/step - loss: 0.3695 - accuracy: 0.8938\n",
      "[CV] END ...........................n_hidden=5, n_neurons=68; total time=  13.9s\n",
      "704/704 [==============================] - 1s 837us/step - loss: 0.3373 - accuracy: 0.8984\n",
      "[CV] END ...........................n_hidden=5, n_neurons=68; total time=  13.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fdf49e932e0>,\n",
       "                   n_iter=3,\n",
       "                   param_distributions={'n_hidden': [4, 5, 6, 7],\n",
       "                                        'n_neurons': [50, 51, 52, 53, 54, 55,\n",
       "                                                      56, 57, 58, 59, 60, 61,\n",
       "                                                      62, 63, 64, 65, 66, 67,\n",
       "                                                      68, 69, 70, 71, 72, 73,\n",
       "                                                      74, 75, 76, 77, 78, 79, ...]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scipy.stats import reciprocal\n",
    "\n",
    "params = {\n",
    "    'n_hidden': np.arange(4, 8).tolist(),\n",
    "    'n_neurons': np.arange(50, 100).tolist(),\n",
    "}\n",
    "\n",
    "rand_searcher = RandomizedSearchCV(mnist_clf, params, n_iter=3, cv=2, verbose=2)\n",
    "rand_searcher.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[EarlyStopping(patience=10)], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neurons': 68, 'n_hidden': 5}\n"
     ]
    }
   ],
   "source": [
    "best_params = rand_searcher.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlearning_rate = 1e-3\\n\\nimport math\\n\\nlearning_rates = [learning_rate]\\nlosses = []\\nwhile learning_rate < 2:\\n    learning_rates.append(learning_rate)\\n    model = build_model(learning_rate=learning_rate, n_neurons=best_params['n_neurons'], n_hidden=best_params['n_hidden'])\\n    model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), verbose=0)\\n    loss = model.evaluate(x_test, y_test)\\n    losses.append(loss)\\n    learning_rate *= math.exp(math.log(10e6) / 500)\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "learning_rate = 1e-3\n",
    "\n",
    "import math\n",
    "\n",
    "learning_rates = [learning_rate]\n",
    "losses = []\n",
    "while learning_rate < 2:\n",
    "    learning_rates.append(learning_rate)\n",
    "    model = build_model(learning_rate=learning_rate, n_neurons=best_params['n_neurons'], n_hidden=best_params['n_hidden'])\n",
    "    model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), verbose=0)\n",
    "    loss = model.evaluate(x_test, y_test)\n",
    "    losses.append(loss)\n",
    "    learning_rate *= math.exp(math.log(10e6) / 500)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "K = backend\n",
    "\n",
    "class ExponentialLearningRate(Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 3s 2ms/step - loss: nan - accuracy: 0.3577 - val_loss: nan - val_accuracy: 0.1001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf19303310>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expon_lr_cb = ExponentialLearningRate(factor=1.005);\n",
    "\n",
    "model = build_model(learning_rate=learning_rate, n_neurons=200, n_hidden=best_params['n_hidden'])\n",
    "model.fit(x_train, y_train, epochs=1, validation_data=(x_val, y_val), callbacks=[expon_lr_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(expon_lr_cb.rates, expon_lr_cb.losses)\n",
    "# plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr_cb.losses), min(expon_lr_cb.rates), max(expon_lr_cb.rates))\n",
    "plt.axis([min(expon_lr_cb.rates), max(expon_lr_cb.rates), 0, expon_lr_cb.losses[0]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimal learning rate is usually 10x less than the lr where the loss starts climbing. Here the lr starts climbing around lr=1.37\n",
    "model = build_model(learning_rate=1.37e-1, n_neurons=200, n_hidden=best_params['n_hidden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, 'logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1407/1407 [==============================] - 3s 2ms/step - loss: 0.6637 - accuracy: 0.7874 - val_loss: 0.1566 - val_accuracy: 0.9523\n",
      "Epoch 2/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.1350 - accuracy: 0.9599 - val_loss: 0.6273 - val_accuracy: 0.8564\n",
      "Epoch 3/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0954 - accuracy: 0.9710 - val_loss: 0.1052 - val_accuracy: 0.9675\n",
      "Epoch 4/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0635 - accuracy: 0.9803 - val_loss: 0.0989 - val_accuracy: 0.9701\n",
      "Epoch 5/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0569 - accuracy: 0.9814 - val_loss: 0.0927 - val_accuracy: 0.9725\n",
      "Epoch 6/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0384 - accuracy: 0.9881 - val_loss: 0.0871 - val_accuracy: 0.9760\n",
      "Epoch 7/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0294 - accuracy: 0.9909 - val_loss: 0.0968 - val_accuracy: 0.9725\n",
      "Epoch 8/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0294 - accuracy: 0.9910 - val_loss: 0.0897 - val_accuracy: 0.9760\n",
      "Epoch 9/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0213 - accuracy: 0.9936 - val_loss: 0.0943 - val_accuracy: 0.9753\n",
      "Epoch 10/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0189 - accuracy: 0.9938 - val_loss: 0.0847 - val_accuracy: 0.9798\n",
      "Epoch 11/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0154 - accuracy: 0.9956 - val_loss: 0.1019 - val_accuracy: 0.9742\n",
      "Epoch 12/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0865 - val_accuracy: 0.9780\n",
      "Epoch 13/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.1098 - val_accuracy: 0.9758\n",
      "Epoch 14/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0158 - accuracy: 0.9953 - val_loss: 0.0898 - val_accuracy: 0.9790\n",
      "Epoch 15/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.1031 - val_accuracy: 0.9780\n",
      "Epoch 16/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.1661 - val_accuracy: 0.9676\n",
      "Epoch 17/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0169 - accuracy: 0.9955 - val_loss: 0.1198 - val_accuracy: 0.9760\n",
      "Epoch 18/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0155 - accuracy: 0.9958 - val_loss: 0.0987 - val_accuracy: 0.9799\n",
      "Epoch 19/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.1438 - val_accuracy: 0.9721\n",
      "Epoch 20/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.1132 - val_accuracy: 0.9761\n",
      "Epoch 21/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0104 - accuracy: 0.9961 - val_loss: 0.1389 - val_accuracy: 0.9733\n",
      "Epoch 22/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.0976 - val_accuracy: 0.9819\n",
      "Epoch 23/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.1212 - val_accuracy: 0.9790\n",
      "Epoch 24/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.1013 - val_accuracy: 0.9806\n",
      "Epoch 25/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.1012 - val_accuracy: 0.9829\n",
      "Epoch 26/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 8.7588e-04 - accuracy: 0.9998 - val_loss: 0.1033 - val_accuracy: 0.9829\n",
      "Epoch 27/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 9.2145e-05 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9833\n",
      "Epoch 28/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 6.1502e-05 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9830\n",
      "Epoch 29/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 4.9883e-05 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9830\n",
      "Epoch 30/60\n",
      "1407/1407 [==============================] - 2s 1ms/step - loss: 3.8558e-05 - accuracy: 1.0000 - val_loss: 0.1094 - val_accuracy: 0.9831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf4b0af2b0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "tensorboard_cb = TensorBoard(get_run_logdir())\n",
    "earlystopping_cb = EarlyStopping(patience=20)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=60, validation_data=(x_val, y_val), callbacks=[tensorboard_cb, earlystopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 679us/step - loss: 0.0994 - accuracy: 0.9838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09943719208240509, 0.9837999939918518]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/\n",
      "    run_2021_04_25-17_51_46/\n",
      "        train/\n",
      "            events.out.tfevents.1619391106.Jamess-MacBook-Pro-2.local.profile-empty\n",
      "            events.out.tfevents.1619391106.Jamess-MacBook-Pro-2.local.11817.5411926.v2\n",
      "            plugins/\n",
      "                profile/\n",
      "                    2021_04_25_17_51_46/\n",
      "                        Jamess-MacBook-Pro-2.local.trace.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.xplane.pb\n",
      "                        Jamess-MacBook-Pro-2.local.overview_page.pb\n",
      "                        Jamess-MacBook-Pro-2.local.input_pipeline.pb\n",
      "                        Jamess-MacBook-Pro-2.local.memory_profile.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.kernel_stats.pb\n",
      "                        Jamess-MacBook-Pro-2.local.tensorflow_stats.pb\n",
      "        validation/\n",
      "            events.out.tfevents.1619391109.Jamess-MacBook-Pro-2.local.11817.5415244.v2\n",
      "    run_2021_04_25-16_55_49/\n",
      "        train/\n",
      "            events.out.tfevents.1619387749.Jamess-MacBook-Pro-2.local.11817.4777798.v2\n",
      "            events.out.tfevents.1619387749.Jamess-MacBook-Pro-2.local.profile-empty\n",
      "            plugins/\n",
      "                profile/\n",
      "                    2021_04_25_16_55_49/\n",
      "                        Jamess-MacBook-Pro-2.local.trace.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.xplane.pb\n",
      "                        Jamess-MacBook-Pro-2.local.overview_page.pb\n",
      "                        Jamess-MacBook-Pro-2.local.input_pipeline.pb\n",
      "                        Jamess-MacBook-Pro-2.local.memory_profile.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.kernel_stats.pb\n",
      "                        Jamess-MacBook-Pro-2.local.tensorflow_stats.pb\n",
      "        validation/\n",
      "            events.out.tfevents.1619387751.Jamess-MacBook-Pro-2.local.11817.4781116.v2\n",
      "    run_2021_04_25-17_43_44/\n",
      "        train/\n",
      "            events.out.tfevents.1619390624.Jamess-MacBook-Pro-2.local.profile-empty\n",
      "            events.out.tfevents.1619390624.Jamess-MacBook-Pro-2.local.11817.5089304.v2\n",
      "            plugins/\n",
      "                profile/\n",
      "                    2021_04_25_17_43_44/\n",
      "                        Jamess-MacBook-Pro-2.local.trace.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.xplane.pb\n",
      "                        Jamess-MacBook-Pro-2.local.overview_page.pb\n",
      "                        Jamess-MacBook-Pro-2.local.input_pipeline.pb\n",
      "                        Jamess-MacBook-Pro-2.local.memory_profile.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.kernel_stats.pb\n",
      "                        Jamess-MacBook-Pro-2.local.tensorflow_stats.pb\n",
      "        validation/\n",
      "            events.out.tfevents.1619390626.Jamess-MacBook-Pro-2.local.11817.5092622.v2\n",
      "    run_2021_04_25-17_46_18/\n",
      "        train/\n",
      "            events.out.tfevents.1619390778.Jamess-MacBook-Pro-2.local.11817.5303468.v2\n",
      "            events.out.tfevents.1619390778.Jamess-MacBook-Pro-2.local.profile-empty\n",
      "            plugins/\n",
      "                profile/\n",
      "                    2021_04_25_17_46_18/\n",
      "                        Jamess-MacBook-Pro-2.local.trace.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.xplane.pb\n",
      "                        Jamess-MacBook-Pro-2.local.overview_page.pb\n",
      "                        Jamess-MacBook-Pro-2.local.input_pipeline.pb\n",
      "                        Jamess-MacBook-Pro-2.local.memory_profile.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.kernel_stats.pb\n",
      "                        Jamess-MacBook-Pro-2.local.tensorflow_stats.pb\n",
      "        validation/\n",
      "            events.out.tfevents.1619390780.Jamess-MacBook-Pro-2.local.11817.5306786.v2\n",
      "    run_2021_04_25-18_02_48/\n",
      "        train/\n",
      "            events.out.tfevents.1619391768.Jamess-MacBook-Pro-2.local.11817.5542460.v2\n",
      "            events.out.tfevents.1619391768.Jamess-MacBook-Pro-2.local.profile-empty\n",
      "            plugins/\n",
      "                profile/\n",
      "                    2021_04_25_18_02_48/\n",
      "                        Jamess-MacBook-Pro-2.local.trace.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.xplane.pb\n",
      "                        Jamess-MacBook-Pro-2.local.overview_page.pb\n",
      "                        Jamess-MacBook-Pro-2.local.input_pipeline.pb\n",
      "                        Jamess-MacBook-Pro-2.local.memory_profile.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.kernel_stats.pb\n",
      "                        Jamess-MacBook-Pro-2.local.tensorflow_stats.pb\n",
      "        validation/\n",
      "            events.out.tfevents.1619391770.Jamess-MacBook-Pro-2.local.11817.5545778.v2\n",
      "    run_2021_04_25-17_39_48/\n",
      "        train/\n",
      "            events.out.tfevents.1619390388.Jamess-MacBook-Pro-2.local.profile-empty\n",
      "            events.out.tfevents.1619390388.Jamess-MacBook-Pro-2.local.11817.4989810.v2\n",
      "            plugins/\n",
      "                profile/\n",
      "                    2021_04_25_17_39_48/\n",
      "                        Jamess-MacBook-Pro-2.local.trace.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.xplane.pb\n",
      "                        Jamess-MacBook-Pro-2.local.overview_page.pb\n",
      "                        Jamess-MacBook-Pro-2.local.input_pipeline.pb\n",
      "                        Jamess-MacBook-Pro-2.local.memory_profile.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.kernel_stats.pb\n",
      "                        Jamess-MacBook-Pro-2.local.tensorflow_stats.pb\n",
      "        validation/\n",
      "            events.out.tfevents.1619390390.Jamess-MacBook-Pro-2.local.11817.4993128.v2\n",
      "    run_2021_04_25-17_44_45/\n",
      "        train/\n",
      "            events.out.tfevents.1619390685.Jamess-MacBook-Pro-2.local.profile-empty\n",
      "            events.out.tfevents.1619390685.Jamess-MacBook-Pro-2.local.11817.5196386.v2\n",
      "            plugins/\n",
      "                profile/\n",
      "                    2021_04_25_17_44_45/\n",
      "                        Jamess-MacBook-Pro-2.local.trace.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.xplane.pb\n",
      "                        Jamess-MacBook-Pro-2.local.overview_page.pb\n",
      "                        Jamess-MacBook-Pro-2.local.input_pipeline.pb\n",
      "                        Jamess-MacBook-Pro-2.local.memory_profile.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.kernel_stats.pb\n",
      "                        Jamess-MacBook-Pro-2.local.tensorflow_stats.pb\n",
      "        validation/\n",
      "            events.out.tfevents.1619390687.Jamess-MacBook-Pro-2.local.11817.5199704.v2\n",
      "    run_2021_04_25-16_57_30/\n",
      "        train/\n",
      "            events.out.tfevents.1619387851.Jamess-MacBook-Pro-2.local.profile-empty\n",
      "            events.out.tfevents.1619387850.Jamess-MacBook-Pro-2.local.11817.4873498.v2\n",
      "            plugins/\n",
      "                profile/\n",
      "                    2021_04_25_16_57_31/\n",
      "                        Jamess-MacBook-Pro-2.local.trace.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.xplane.pb\n",
      "                        Jamess-MacBook-Pro-2.local.overview_page.pb\n",
      "                        Jamess-MacBook-Pro-2.local.input_pipeline.pb\n",
      "                        Jamess-MacBook-Pro-2.local.memory_profile.json.gz\n",
      "                        Jamess-MacBook-Pro-2.local.kernel_stats.pb\n",
      "                        Jamess-MacBook-Pro-2.local.tensorflow_stats.pb\n",
      "        validation/\n",
      "            events.out.tfevents.1619387852.Jamess-MacBook-Pro-2.local.11817.4876816.v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "            \n",
    "list_files(\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
