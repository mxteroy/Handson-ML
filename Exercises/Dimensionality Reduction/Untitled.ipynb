{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The benefits of reducing the dimensions of a dataset is that training speeds are sped up and it makes it possible to graph high deimension data. The higher the dimensions (# of features) the greater the risk of overfitting. The main drawbacks are that information is lost (akin to compressing files) which means the system will perform slightly worse, and the code gets more complex because of larger pipelines. \n",
    "\n",
    "2. The **curse of dimensionality** is that the higher the dimensions, the greater the risk of overfitting is. \n",
    "\n",
    "3. It is almost impossible to perfectly revert back to the original dataset prior to dimensionality reduction because of loss of information when reducing dimensions. PCA has a procedure that reverses dim-reduction.\n",
    "\n",
    "4. Yes it can because the goal of PCA is to get rid of the useless dimensions without losing too much information. \n",
    "\n",
    "5. ~~Around 150 dimensions accroding to the graph on figure 8-8 explained variance vs # of dims~~ The number of dimensions required to preserve a certain percentage of the variance varies with the dataset. For example, one dimension could be enough to preserve 95% of the variance on a particular dataset, while on another dataset with data in perfectly random points, there it would take 95% of the dataset's instances to preserve 95% variance. \n",
    "\n",
    "6. \n",
    "    - regular PCA: the default option. Use if dataset fits in memory\n",
    "    - Incremental PCA: use if dataset **does not** fit in memory. Basically online PCA. Slower that regular PCA\n",
    "    - Randomized PCA: use when to goal dimension is significantly lower than the original and there is enough memory for the dataset\n",
    "    - Kernel PCA: for nonlinear datasets\n",
    "    \n",
    "7. To evaluate dimensionality reduction, compute the MSE of the reconstruction pre-image of the reduced dataset with the original dataset (reconstruction error) if the dim-reduce technique has reconstruction methods. If not, then train a model using the reduced dataset and measure its performance.\n",
    "\n",
    "8. Chaining different dim-reduction algorithms makes sense. For example, PCA can be used to initially prune out the useless dimensions and then a much slower dim-reduction algo like LLE. This will speed up the time to reduce the dataset can still preserve the same variance as just solely using LLE.\n",
    "\n",
    "### 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1617, 64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.2374420166015625s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "rf_clf1 = RandomForestClassifier(max_depth=10, random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "rf_clf1.fit(X_train, y_train)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833333333333333"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_reducer = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1617, 29)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = pca_reducer.fit_transform(X_train)\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.4412369728088379s\n"
     ]
    }
   ],
   "source": [
    "rf_clf2 = RandomForestClassifier(max_depth=10, random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "rf_clf2.fit(X_train_new, y_train)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time is actually almost twice as slow!!! This is an example that **dimensionality reduction does not always lead to faster training time.** *It really depends on the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 29)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_new = pca_reducer.transform(X_test)\n",
    "print(X_test_new.shape)\n",
    "rf_clf2.score(X_test_new, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lower accuracy because of loss of information from dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dimensionality can make models train faster depending on the dataset. Here's an example of speed improvement from dim-reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.08647704124450684s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameseroy/ml/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "soft_clf1 = LogisticRegression(multi_class=\"multinomial\", random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "soft_clf1.fit(X_train, y_train)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_clf1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.08101677894592285s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameseroy/ml/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "soft_clf2 = LogisticRegression(multi_class=\"multinomial\", random_state=42)\n",
    "\n",
    "start = time.time()\n",
    "soft_clf2.fit(X_train_new, y_train)\n",
    "stop = time.time()\n",
    "print(f\"Training time: {stop - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_clf2.score(X_test_new, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
