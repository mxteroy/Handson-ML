{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines pg. 167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The fundamental idea behind an SVM is that it tries to fit the \"widest street possible\" between different classes as a decision boundary.\n",
    "    - Soft margin classification finds the balance between keeping the largest street possible and limiting the margin violations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Support vectors are what determine the decision boundary and keep the \"street\" as wide as possible. They are the edges of the street. Any instance outside the support vectors will not affect the decision boundary (DB). In soft margin classifcation, any instances that is inside the DB will change the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. It is important to scale the input because if the features are not scaled, then the \"street\" that SVM defines will be parallel (or almost) or vertical to the x-axis because SVM will neglect smaller features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SVM can output the distance between an instance and the decision boundary. This can be used as the confidence score. Though, this scores cannot be directly converted to the predicted class probability. (unless Scikit's SVC classifier is used with hyperparameter <i>probability=true</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Both the primal an dual form of the linear SVM problem will yield the same solution, but the primal form of the linear SVM objective is faster when the number of training instance is greater than the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. higher gamma leads to overfitting. If an SVM classifier is underfitting, then gamma should be decreased and/or C should be decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = load_iris() # setosa and veriscolor classes are linearly separable\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]\n",
    "\n",
    "C = 5\n",
    "alpha = 1 / (C * len(X))\n",
    "\n",
    "lin_svc = LinearSVC(loss=\"hinge\", C=C) #default loss is squared hinge (change to hinge)\n",
    "sgd_clf = SGDClassifier(random_state=42, learning_rate=\"constant\", eta0=0.001, alpha=alpha, max_iter=100000, tol=-np.infty) #default loss is hinge\n",
    "svc_clf = SVC(kernel=\"linear\", C=C) #default kernel is rbf (change to linear b/c the dataset is linearly separable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lin_svc.fit(X_scaled, y)\n",
    "sgd_clf.fit(X_scaled, y)\n",
    "svc_clf.fit(X_scaled, y)\n",
    "\n",
    "lin_svc_score = cross_val_score(lin_svc, X_test, y_test)\n",
    "sgd_clf_score = cross_val_score(sgd_clf, X_test, y_test)\n",
    "svc_clf_score = cross_val_score(sgd_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_svc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores are 100% because the dataset is linearly separable. To compare the similarity of these two models, their decision boundaries must be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC:                    [0.20728759] [[ 0.20698778 -0.31798522  0.67690973  0.80207236]]\n",
      "SVC:                          [0.24282611] [[ 0.26732082 -0.33928505  0.69992451  0.74910678]]\n",
      "SGDClassifier(alpha=0.00200): [0.244] [[ 0.26765984 -0.33920468  0.70326094  0.75104712]]\n"
     ]
    }
   ],
   "source": [
    "print(\"LinearSVC:                   \", lin_svc.intercept_, lin_svc.coef_)\n",
    "print(\"SVC:                         \", svc_clf.intercept_, svc_clf.coef_)\n",
    "print(\"SGDClassifier(alpha={:.5f}):\".format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
