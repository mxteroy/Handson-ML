{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is not okay to make the weights all the same because this **causes symmetry** that backpropagation cannot break. All neurons in layers after the initial layers will have the same weights which makes it seem like there's just one neuron per layer + slower. Impossible to converge to a good solution\n",
    "2. Does not matter is biases are intialized to zero or not\n",
    "3. The SELU function will:\n",
    "    a. allow the network to self-normalize if all layers are dense and use the SELU activation function\n",
    "    b. Much faster\n",
    "    c. Always has a non-zero derivate\n",
    "    d. can output negative values so that the mean output is closer to 0 (better for mitigating the risk of vanishing gradients) while ReLU has a mean closer to 0.5. \n",
    "4. \n",
    "    a. SELU for general use cases\n",
    "    b. ELU for network's whose architecture prevent it from normalizing\n",
    "    c. Leak ReLU if trying to minimize runtime latency and don't want to tweak another hyperparameter \n",
    "    d. RReLU if there's time and computing power\n",
    "    e. PReLU if training set is huge.\n",
    "    f. Vanilla ReLU for popularity\n",
    "    g. tanh for output between -1 and 1\n",
    "    h. logistic sigmoid for estimating a probaility, rarely used in hidden layers\n",
    "    j. softmax for outputting probabilities for mutually exclusive classes, rarely used in hidden layers\n",
    "5. Setting the momentum hyperparameter too close to 1 will not effectively prevent the momentum from becoming too large so the gradient descent algorithm will oscillate many many times before converging because the learning rate won't let it sit reach the minima\n",
    "6. Three ways to produce a sparse model:\n",
    "    a. do normal training and make the tiny weights zero\n",
    "    b. l<sub>1</sub> regularization\n",
    "    c. use the TF Model Optimization Toolkit\n",
    "7. Dropout slows down training, but makes the model perform better generally. There is no impact on inference/predicting speed because it is only use during training. MC Dropout is the same as regular Dropout but is active during inference and slows down the network; it must be run 10 times or more for better predictions so it is slower by a factor of at least 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(layers.Dense,activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "model = models.Sequential(\n",
    "    [layers.Flatten(input_shape=[32, 32, 3])] +\n",
    "    [RegularizedDense(100) for _ in range(20)] + \n",
    "    [layers.Dense(10, activation=\"softmax\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_47 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_1335 (Dense)           (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_1336 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1337 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1338 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1339 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1340 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1341 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1342 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1343 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1344 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1345 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1346 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1347 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1348 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1349 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1350 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1351 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1352 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1353 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1354 (Dense)           (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1355 (Dense)           (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "train, test = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train[0], train[1]\n",
    "x_test, y_test = test[0], test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1047/1047 [==============================] - 8s 5ms/step - loss: 11.8359 - accuracy: 0.1376 - val_loss: 2.1895 - val_accuracy: 0.2047\n",
      "Epoch 2/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 2.1281 - accuracy: 0.2188 - val_loss: 2.0379 - val_accuracy: 0.2496\n",
      "Epoch 3/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 2.0063 - accuracy: 0.2611 - val_loss: 1.9434 - val_accuracy: 0.2692\n",
      "Epoch 4/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.9286 - accuracy: 0.2857 - val_loss: 1.9211 - val_accuracy: 0.2788\n",
      "Epoch 5/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.8805 - accuracy: 0.3068 - val_loss: 1.9584 - val_accuracy: 0.2815\n",
      "Epoch 6/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.8145 - accuracy: 0.3390 - val_loss: 1.8355 - val_accuracy: 0.3325\n",
      "Epoch 7/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.7705 - accuracy: 0.3531 - val_loss: 1.7649 - val_accuracy: 0.3644\n",
      "Epoch 8/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.7208 - accuracy: 0.3792 - val_loss: 1.7577 - val_accuracy: 0.3599\n",
      "Epoch 9/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.6896 - accuracy: 0.3867 - val_loss: 1.7394 - val_accuracy: 0.3721\n",
      "Epoch 10/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.6583 - accuracy: 0.3961 - val_loss: 1.7013 - val_accuracy: 0.3818\n",
      "Epoch 11/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.6265 - accuracy: 0.4157 - val_loss: 1.7303 - val_accuracy: 0.3663\n",
      "Epoch 12/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.6097 - accuracy: 0.4155 - val_loss: 1.6429 - val_accuracy: 0.4032\n",
      "Epoch 13/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.5774 - accuracy: 0.4379 - val_loss: 1.6390 - val_accuracy: 0.4063\n",
      "Epoch 14/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.5516 - accuracy: 0.4400 - val_loss: 1.6161 - val_accuracy: 0.4169\n",
      "Epoch 15/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.5383 - accuracy: 0.4439 - val_loss: 1.6369 - val_accuracy: 0.4124\n",
      "Epoch 16/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.5155 - accuracy: 0.4535 - val_loss: 1.5896 - val_accuracy: 0.4250\n",
      "Epoch 17/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.4984 - accuracy: 0.4583 - val_loss: 1.6007 - val_accuracy: 0.4301\n",
      "Epoch 18/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.4870 - accuracy: 0.4640 - val_loss: 1.5664 - val_accuracy: 0.4358\n",
      "Epoch 19/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.4607 - accuracy: 0.4731 - val_loss: 1.6017 - val_accuracy: 0.4230\n",
      "Epoch 20/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.4430 - accuracy: 0.4806 - val_loss: 1.6252 - val_accuracy: 0.4223\n",
      "Epoch 21/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.4405 - accuracy: 0.4805 - val_loss: 1.5649 - val_accuracy: 0.4348\n",
      "Epoch 22/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.4190 - accuracy: 0.4872 - val_loss: 1.6863 - val_accuracy: 0.4015\n",
      "Epoch 23/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.4143 - accuracy: 0.4958 - val_loss: 1.5686 - val_accuracy: 0.4371\n",
      "Epoch 24/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.4039 - accuracy: 0.4954 - val_loss: 1.5472 - val_accuracy: 0.4442\n",
      "Epoch 25/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.3756 - accuracy: 0.5047 - val_loss: 1.5471 - val_accuracy: 0.4490\n",
      "Epoch 26/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.3661 - accuracy: 0.5082 - val_loss: 1.5928 - val_accuracy: 0.4332\n",
      "Epoch 27/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.3641 - accuracy: 0.5108 - val_loss: 1.5375 - val_accuracy: 0.4492\n",
      "Epoch 28/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.3504 - accuracy: 0.5132 - val_loss: 1.5391 - val_accuracy: 0.4529\n",
      "Epoch 29/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.3405 - accuracy: 0.5177 - val_loss: 1.5492 - val_accuracy: 0.4525\n",
      "Epoch 30/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.3390 - accuracy: 0.5179 - val_loss: 1.5505 - val_accuracy: 0.4495\n",
      "Epoch 31/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.3243 - accuracy: 0.5285 - val_loss: 1.5607 - val_accuracy: 0.4472\n",
      "Epoch 32/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.3020 - accuracy: 0.5363 - val_loss: 1.5747 - val_accuracy: 0.4446\n",
      "Epoch 33/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.2952 - accuracy: 0.5352 - val_loss: 1.5387 - val_accuracy: 0.4583\n",
      "Epoch 34/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.2932 - accuracy: 0.5417 - val_loss: 1.5434 - val_accuracy: 0.4547\n",
      "Epoch 35/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.2705 - accuracy: 0.5455 - val_loss: 1.5602 - val_accuracy: 0.4528\n",
      "Epoch 36/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.2563 - accuracy: 0.5498 - val_loss: 1.5566 - val_accuracy: 0.4590\n",
      "Epoch 37/50\n",
      "1047/1047 [==============================] - 5s 5ms/step - loss: 1.2533 - accuracy: 0.5527 - val_loss: 1.5813 - val_accuracy: 0.4477\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.5225 - accuracy: 0.4568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5225147008895874, 0.45680001378059387]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "early_stop_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# s = 20 * len(x_train) // 32\n",
    "# learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "\n",
    "optimizer = optimizers.Nadam(learning_rate=5e-5) # does not support learnign rate scheduling\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=[early_stop_cb])\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1047/1047 [==============================] - 19s 9ms/step - loss: 2.0688 - accuracy: 0.2638 - val_loss: 1.7308 - val_accuracy: 0.3802\n",
      "Epoch 2/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.7583 - accuracy: 0.3724 - val_loss: 1.6471 - val_accuracy: 0.4140\n",
      "Epoch 3/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.6772 - accuracy: 0.4011 - val_loss: 1.5862 - val_accuracy: 0.4320\n",
      "Epoch 4/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.6348 - accuracy: 0.4150 - val_loss: 1.5553 - val_accuracy: 0.4435\n",
      "Epoch 5/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.5991 - accuracy: 0.4308 - val_loss: 1.5368 - val_accuracy: 0.4544\n",
      "Epoch 6/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.5493 - accuracy: 0.4529 - val_loss: 1.5079 - val_accuracy: 0.4618\n",
      "Epoch 7/50\n",
      "1047/1047 [==============================] - 9s 9ms/step - loss: 1.5189 - accuracy: 0.4595 - val_loss: 1.4983 - val_accuracy: 0.4649\n",
      "Epoch 8/50\n",
      "1047/1047 [==============================] - 9s 9ms/step - loss: 1.5072 - accuracy: 0.4675 - val_loss: 1.4784 - val_accuracy: 0.4709\n",
      "Epoch 9/50\n",
      "1047/1047 [==============================] - 9s 9ms/step - loss: 1.4788 - accuracy: 0.4781 - val_loss: 1.4609 - val_accuracy: 0.4808\n",
      "Epoch 10/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.4509 - accuracy: 0.4898 - val_loss: 1.4590 - val_accuracy: 0.4808\n",
      "Epoch 11/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.4423 - accuracy: 0.4885 - val_loss: 1.4486 - val_accuracy: 0.4843\n",
      "Epoch 12/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.4240 - accuracy: 0.4949 - val_loss: 1.4461 - val_accuracy: 0.4864\n",
      "Epoch 13/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.3997 - accuracy: 0.5021 - val_loss: 1.4357 - val_accuracy: 0.4915\n",
      "Epoch 14/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.3777 - accuracy: 0.5128 - val_loss: 1.4338 - val_accuracy: 0.4913\n",
      "Epoch 15/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.3634 - accuracy: 0.5177 - val_loss: 1.4206 - val_accuracy: 0.4967\n",
      "Epoch 16/50\n",
      "1047/1047 [==============================] - 10s 9ms/step - loss: 1.3457 - accuracy: 0.5272 - val_loss: 1.4189 - val_accuracy: 0.4981\n",
      "Epoch 17/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.3409 - accuracy: 0.5252 - val_loss: 1.4136 - val_accuracy: 0.5004\n",
      "Epoch 18/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.3175 - accuracy: 0.5330 - val_loss: 1.4185 - val_accuracy: 0.4981\n",
      "Epoch 19/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.3181 - accuracy: 0.5359 - val_loss: 1.4134 - val_accuracy: 0.4984\n",
      "Epoch 20/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.3025 - accuracy: 0.5400 - val_loss: 1.4025 - val_accuracy: 0.5058\n",
      "Epoch 21/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.2877 - accuracy: 0.5454 - val_loss: 1.4083 - val_accuracy: 0.5035\n",
      "Epoch 22/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.2839 - accuracy: 0.5439 - val_loss: 1.4088 - val_accuracy: 0.5095\n",
      "Epoch 23/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.2639 - accuracy: 0.5532 - val_loss: 1.4038 - val_accuracy: 0.5053\n",
      "Epoch 24/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.2537 - accuracy: 0.5575 - val_loss: 1.4076 - val_accuracy: 0.5039\n",
      "Epoch 25/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.2348 - accuracy: 0.5632 - val_loss: 1.4006 - val_accuracy: 0.5071\n",
      "Epoch 26/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.2393 - accuracy: 0.5638 - val_loss: 1.4112 - val_accuracy: 0.5070\n",
      "Epoch 27/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.2073 - accuracy: 0.5740 - val_loss: 1.4140 - val_accuracy: 0.5044\n",
      "Epoch 28/50\n",
      "1047/1047 [==============================] - 9s 9ms/step - loss: 1.2181 - accuracy: 0.5691 - val_loss: 1.4088 - val_accuracy: 0.5114\n",
      "Epoch 29/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.2207 - accuracy: 0.5663 - val_loss: 1.4106 - val_accuracy: 0.5104\n",
      "Epoch 30/50\n",
      "1047/1047 [==============================] - 9s 9ms/step - loss: 1.1923 - accuracy: 0.5817 - val_loss: 1.3974 - val_accuracy: 0.5119\n",
      "Epoch 31/50\n",
      "1047/1047 [==============================] - 9s 9ms/step - loss: 1.1920 - accuracy: 0.5764 - val_loss: 1.4073 - val_accuracy: 0.5133\n",
      "Epoch 32/50\n",
      "1047/1047 [==============================] - 9s 9ms/step - loss: 1.1758 - accuracy: 0.5861 - val_loss: 1.4163 - val_accuracy: 0.5105\n",
      "Epoch 33/50\n",
      "1047/1047 [==============================] - 9s 9ms/step - loss: 1.1490 - accuracy: 0.5951 - val_loss: 1.4304 - val_accuracy: 0.5039\n",
      "Epoch 34/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.1599 - accuracy: 0.5951 - val_loss: 1.4016 - val_accuracy: 0.5128\n",
      "Epoch 35/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.1375 - accuracy: 0.6014 - val_loss: 1.4156 - val_accuracy: 0.5107\n",
      "Epoch 36/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.1306 - accuracy: 0.6002 - val_loss: 1.4198 - val_accuracy: 0.5116\n",
      "Epoch 37/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.1284 - accuracy: 0.6044 - val_loss: 1.4311 - val_accuracy: 0.5059\n",
      "Epoch 38/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.1240 - accuracy: 0.6016 - val_loss: 1.4252 - val_accuracy: 0.5040\n",
      "Epoch 39/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.1123 - accuracy: 0.6057 - val_loss: 1.4248 - val_accuracy: 0.5123\n",
      "Epoch 40/50\n",
      "1047/1047 [==============================] - 9s 8ms/step - loss: 1.1086 - accuracy: 0.6086 - val_loss: 1.4221 - val_accuracy: 0.5122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc35cb1640>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "RegularizedDense = partial(layers.Dense,activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "\n",
    "model_BN = models.Sequential()\n",
    "model_BN.add(layers.Flatten(input_shape=x_train.shape[1:]))\n",
    "model_BN.add(layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model_BN.add(layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model_BN.add(layers.BatchNormalization())\n",
    "    model_BN.add(keras.layers.Activation(\"elu\"))\n",
    "    \n",
    "model_BN.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "early_stop_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model_BN_checkpoint_cb = ModelCheckpoint(\"cifar10_bn_model.h5\", save_best_only=True)\n",
    "model_BN.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_BN.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, callbacks=[early_stop_cb, model_BN_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a network using BN converges much faster than the previous network. From the get-go the BN network had a lower loss than the previous network\n",
    "- the BN network also produces a better model!\n",
    "- Batch normalization (BN) adds extra computation to the network so training time per epoch is actually longer than the training times for the previous network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 2.2673 - accuracy: 0.1628 - val_loss: 2.0264 - val_accuracy: 0.2427\n",
      "Epoch 2/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.9387 - accuracy: 0.2746 - val_loss: 1.8717 - val_accuracy: 0.3048\n",
      "Epoch 3/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.8513 - accuracy: 0.3124 - val_loss: 1.8288 - val_accuracy: 0.3259\n",
      "Epoch 4/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.8029 - accuracy: 0.3395 - val_loss: 1.7872 - val_accuracy: 0.3424\n",
      "Epoch 5/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.7773 - accuracy: 0.3476 - val_loss: 1.7768 - val_accuracy: 0.3470\n",
      "Epoch 6/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.7259 - accuracy: 0.3672 - val_loss: 1.7146 - val_accuracy: 0.3685\n",
      "Epoch 7/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.7194 - accuracy: 0.3757 - val_loss: 1.7069 - val_accuracy: 0.3859\n",
      "Epoch 8/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.6804 - accuracy: 0.3859 - val_loss: 1.7331 - val_accuracy: 0.3716\n",
      "Epoch 9/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.6456 - accuracy: 0.3972 - val_loss: 1.7233 - val_accuracy: 0.3804\n",
      "Epoch 10/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.6319 - accuracy: 0.4142 - val_loss: 1.6747 - val_accuracy: 0.3932\n",
      "Epoch 11/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.5917 - accuracy: 0.4248 - val_loss: 1.6264 - val_accuracy: 0.4187\n",
      "Epoch 12/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.5787 - accuracy: 0.4279 - val_loss: 1.6041 - val_accuracy: 0.4252\n",
      "Epoch 13/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.5695 - accuracy: 0.4350 - val_loss: 1.6995 - val_accuracy: 0.4009\n",
      "Epoch 14/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.5712 - accuracy: 0.4373 - val_loss: 1.5817 - val_accuracy: 0.4333\n",
      "Epoch 15/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.5552 - accuracy: 0.4421 - val_loss: 1.5749 - val_accuracy: 0.4299\n",
      "Epoch 16/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.5221 - accuracy: 0.4517 - val_loss: 1.5930 - val_accuracy: 0.4218\n",
      "Epoch 17/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.5292 - accuracy: 0.4512 - val_loss: 1.6119 - val_accuracy: 0.4278\n",
      "Epoch 18/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4960 - accuracy: 0.4636 - val_loss: 1.6114 - val_accuracy: 0.4290\n",
      "Epoch 19/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4966 - accuracy: 0.4659 - val_loss: 1.5864 - val_accuracy: 0.4300\n",
      "Epoch 20/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4769 - accuracy: 0.4730 - val_loss: 1.5802 - val_accuracy: 0.4339\n",
      "Epoch 21/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4664 - accuracy: 0.4672 - val_loss: 1.5701 - val_accuracy: 0.4346\n",
      "Epoch 22/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4620 - accuracy: 0.4729 - val_loss: 1.5653 - val_accuracy: 0.4465\n",
      "Epoch 23/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4563 - accuracy: 0.4826 - val_loss: 1.5679 - val_accuracy: 0.4419\n",
      "Epoch 24/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4412 - accuracy: 0.4889 - val_loss: 1.5676 - val_accuracy: 0.4451\n",
      "Epoch 25/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4220 - accuracy: 0.4875 - val_loss: 1.6101 - val_accuracy: 0.4343\n",
      "Epoch 26/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4249 - accuracy: 0.4876 - val_loss: 1.6075 - val_accuracy: 0.4282\n",
      "Epoch 27/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4054 - accuracy: 0.4988 - val_loss: 1.5438 - val_accuracy: 0.4564\n",
      "Epoch 28/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4108 - accuracy: 0.4987 - val_loss: 1.6084 - val_accuracy: 0.4288\n",
      "Epoch 29/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4003 - accuracy: 0.4985 - val_loss: 1.5648 - val_accuracy: 0.4540\n",
      "Epoch 30/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3809 - accuracy: 0.5094 - val_loss: 1.5835 - val_accuracy: 0.4460\n",
      "Epoch 31/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3846 - accuracy: 0.5039 - val_loss: 1.5538 - val_accuracy: 0.4537\n",
      "Epoch 32/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3846 - accuracy: 0.5043 - val_loss: 1.5295 - val_accuracy: 0.4607\n",
      "Epoch 33/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3606 - accuracy: 0.5128 - val_loss: 1.5323 - val_accuracy: 0.4528\n",
      "Epoch 34/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3556 - accuracy: 0.5159 - val_loss: 1.5463 - val_accuracy: 0.4524\n",
      "Epoch 35/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3407 - accuracy: 0.5214 - val_loss: 1.5520 - val_accuracy: 0.4552\n",
      "Epoch 36/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3369 - accuracy: 0.5230 - val_loss: 1.5672 - val_accuracy: 0.4509\n",
      "Epoch 37/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3269 - accuracy: 0.5223 - val_loss: 1.5347 - val_accuracy: 0.4660\n",
      "Epoch 38/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3251 - accuracy: 0.5293 - val_loss: 1.5618 - val_accuracy: 0.4575\n",
      "Epoch 39/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3021 - accuracy: 0.5371 - val_loss: 1.5537 - val_accuracy: 0.4617\n",
      "Epoch 40/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3073 - accuracy: 0.5315 - val_loss: 1.5506 - val_accuracy: 0.4583\n",
      "Epoch 41/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3003 - accuracy: 0.5384 - val_loss: 1.5467 - val_accuracy: 0.4638\n",
      "Epoch 42/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.2979 - accuracy: 0.5372 - val_loss: 1.5630 - val_accuracy: 0.4605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc41934910>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import LecunNormal\n",
    "import os\n",
    "\n",
    "model_self_norm = models.Sequential()\n",
    "model_self_norm.add(layers.Flatten(input_shape=x_train.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model_self_norm.add(layers.Dense(100, activation='selu', kernel_initializer=LecunNormal()))\n",
    "model_self_norm.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "early_stop_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model_self_norm_checkpoint_cb = ModelCheckpoint(\"cifar10_self_norm_model.h5\", save_best_only=True)\n",
    "model_self_norm.compile(optimizer=optimizers.SGD(lr=0.001, momentum = 0.9), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_self_norm.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, callbacks=[early_stop_cb, model_self_norm_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1047/1047 [==============================] - 4s 3ms/step - loss: 2.1381 - accuracy: 0.2551 - val_loss: 1.7954 - val_accuracy: 0.3647\n",
      "Epoch 2/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.6971 - accuracy: 0.3925 - val_loss: 1.6520 - val_accuracy: 0.4258\n",
      "Epoch 3/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.5761 - accuracy: 0.4468 - val_loss: 1.6753 - val_accuracy: 0.4145\n",
      "Epoch 4/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4829 - accuracy: 0.4752 - val_loss: 1.5773 - val_accuracy: 0.4480\n",
      "Epoch 5/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.4291 - accuracy: 0.4908 - val_loss: 1.5667 - val_accuracy: 0.4630\n",
      "Epoch 6/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3599 - accuracy: 0.5182 - val_loss: 1.5379 - val_accuracy: 0.4615\n",
      "Epoch 7/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.3240 - accuracy: 0.5333 - val_loss: 1.5529 - val_accuracy: 0.4793\n",
      "Epoch 8/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.2771 - accuracy: 0.5438 - val_loss: 1.5560 - val_accuracy: 0.4742\n",
      "Epoch 9/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.2318 - accuracy: 0.5642 - val_loss: 1.5756 - val_accuracy: 0.4805\n",
      "Epoch 10/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.1909 - accuracy: 0.5775 - val_loss: 1.5917 - val_accuracy: 0.4746\n",
      "Epoch 11/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.1613 - accuracy: 0.5891 - val_loss: 1.6149 - val_accuracy: 0.4856\n",
      "Epoch 12/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.1324 - accuracy: 0.5978 - val_loss: 1.6150 - val_accuracy: 0.4936\n",
      "Epoch 13/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.1008 - accuracy: 0.6102 - val_loss: 1.6532 - val_accuracy: 0.4916\n",
      "Epoch 14/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.0469 - accuracy: 0.6303 - val_loss: 1.6909 - val_accuracy: 0.4858\n",
      "Epoch 15/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 1.0172 - accuracy: 0.6391 - val_loss: 1.7132 - val_accuracy: 0.4899\n",
      "Epoch 16/50\n",
      "1047/1047 [==============================] - 3s 3ms/step - loss: 0.9961 - accuracy: 0.6443 - val_loss: 1.7065 - val_accuracy: 0.4824\n"
     ]
    }
   ],
   "source": [
    "model_dropout = models.Sequential()\n",
    "model_dropout.add(layers.Flatten(input_shape=x_train.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model_dropout.add(layers.Dense(100, activation='selu', kernel_initializer=LecunNormal()))\n",
    "model_dropout.add(layers.AlphaDropout(rate=0.1)) # why is it only at the end of the network?\n",
    "model_dropout.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "early_stop_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model_dropout_checkpoint_cb = ModelCheckpoint(\"cifar10_self_norm_model.h5\", save_best_only=True)\n",
    "model_dropout.compile(optimizer=optimizers.SGD(lr=0.001, momentum = 0.9), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "x_means = x_train.mean(axis=0)\n",
    "x_stds = x_train.std(axis=0)\n",
    "x_train_scaled = (x_train - x_means) / x_stds\n",
    "x_val_scaled = (x_val - x_means) / x_stds\n",
    "x_test_scaled = (x_test - x_means) / x_stds\n",
    "\n",
    "history_dropout = model_dropout.fit(x_train_scaled, y_train, validation_data=(x_val_scaled, y_val), epochs=50, callbacks=[early_stop_cb, model_dropout_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc_drop = models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer for layer in model_dropout.layers #replace alphadropout layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define utility functions\n",
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4624848484848485"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mc_dropout_predict_classes(model_mc_drop, x_val_scaled)\n",
    "accuracy = np.mean(y_pred == y_val[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cycle scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "262/262 [==============================] - 3s 8ms/step - loss: 0.7886 - accuracy: 0.7494 - val_loss: 1.7992 - val_accuracy: 0.5064\n",
      "Epoch 2/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5985 - accuracy: 0.8078 - val_loss: 1.8723 - val_accuracy: 0.5090\n",
      "Epoch 3/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5299 - accuracy: 0.8312 - val_loss: 1.9676 - val_accuracy: 0.5072\n",
      "Epoch 4/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5143 - accuracy: 0.8363 - val_loss: 2.0312 - val_accuracy: 0.5050\n",
      "Epoch 5/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5021 - accuracy: 0.8401 - val_loss: 1.9744 - val_accuracy: 0.4976\n",
      "Epoch 6/45\n",
      "262/262 [==============================] - 2s 8ms/step - loss: 0.5182 - accuracy: 0.8333 - val_loss: 1.9926 - val_accuracy: 0.4985\n",
      "Epoch 7/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5434 - accuracy: 0.8284 - val_loss: 1.9091 - val_accuracy: 0.4985\n",
      "Epoch 8/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5746 - accuracy: 0.8169 - val_loss: 1.8521 - val_accuracy: 0.4959\n",
      "Epoch 9/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.6111 - accuracy: 0.8042 - val_loss: 2.0288 - val_accuracy: 0.4921\n",
      "Epoch 10/45\n",
      "262/262 [==============================] - 2s 8ms/step - loss: 0.6466 - accuracy: 0.7943 - val_loss: 1.8307 - val_accuracy: 0.4940\n",
      "Epoch 11/45\n",
      "262/262 [==============================] - 2s 8ms/step - loss: 0.6659 - accuracy: 0.7884 - val_loss: 1.8312 - val_accuracy: 0.4896\n",
      "Epoch 12/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.6988 - accuracy: 0.7780 - val_loss: 1.8743 - val_accuracy: 0.4880\n",
      "Epoch 13/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.7664 - accuracy: 0.7566 - val_loss: 1.8089 - val_accuracy: 0.4854\n",
      "Epoch 14/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.7645 - accuracy: 0.7558 - val_loss: 1.7825 - val_accuracy: 0.4857\n",
      "Epoch 15/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.8143 - accuracy: 0.7437 - val_loss: 1.9035 - val_accuracy: 0.4778\n",
      "Epoch 16/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.8671 - accuracy: 0.7279 - val_loss: 1.9307 - val_accuracy: 0.4718\n",
      "Epoch 17/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.9454 - accuracy: 0.7015 - val_loss: 1.7553 - val_accuracy: 0.4688\n",
      "Epoch 18/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.9340 - accuracy: 0.7027 - val_loss: 1.7885 - val_accuracy: 0.4792\n",
      "Epoch 19/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.9835 - accuracy: 0.6881 - val_loss: 1.7477 - val_accuracy: 0.4802\n",
      "Epoch 20/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 1.0931 - accuracy: 0.6558 - val_loss: 1.8051 - val_accuracy: 0.4750\n",
      "Epoch 21/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 1.0787 - accuracy: 0.6611 - val_loss: 1.8077 - val_accuracy: 0.4499\n",
      "Epoch 22/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 1.1282 - accuracy: 0.6421 - val_loss: 1.7669 - val_accuracy: 0.4712\n",
      "Epoch 23/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 1.1681 - accuracy: 0.6277 - val_loss: 1.7680 - val_accuracy: 0.4853\n",
      "Epoch 24/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 1.0573 - accuracy: 0.6679 - val_loss: 1.7569 - val_accuracy: 0.4761\n",
      "Epoch 25/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 1.0219 - accuracy: 0.6781 - val_loss: 1.7969 - val_accuracy: 0.4805\n",
      "Epoch 26/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.9100 - accuracy: 0.7095 - val_loss: 1.8323 - val_accuracy: 0.4767\n",
      "Epoch 27/45\n",
      "262/262 [==============================] - 2s 8ms/step - loss: 0.8409 - accuracy: 0.7331 - val_loss: 1.8524 - val_accuracy: 0.4865\n",
      "Epoch 28/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.7840 - accuracy: 0.7521 - val_loss: 1.7845 - val_accuracy: 0.4954\n",
      "Epoch 29/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.7237 - accuracy: 0.7718 - val_loss: 1.8620 - val_accuracy: 0.4992\n",
      "Epoch 30/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.6877 - accuracy: 0.7830 - val_loss: 1.9586 - val_accuracy: 0.4980\n",
      "Epoch 31/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.6262 - accuracy: 0.8044 - val_loss: 2.0288 - val_accuracy: 0.4987\n",
      "Epoch 32/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5784 - accuracy: 0.8195 - val_loss: 2.0920 - val_accuracy: 0.5004\n",
      "Epoch 33/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5427 - accuracy: 0.8294 - val_loss: 2.0589 - val_accuracy: 0.4993\n",
      "Epoch 34/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.5058 - accuracy: 0.8456 - val_loss: 2.1302 - val_accuracy: 0.4987\n",
      "Epoch 35/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.4565 - accuracy: 0.8592 - val_loss: 2.1576 - val_accuracy: 0.4979\n",
      "Epoch 36/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.4243 - accuracy: 0.8678 - val_loss: 2.2696 - val_accuracy: 0.5001\n",
      "Epoch 37/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.3853 - accuracy: 0.8810 - val_loss: 2.3400 - val_accuracy: 0.4973\n",
      "Epoch 38/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.3553 - accuracy: 0.8898 - val_loss: 2.3882 - val_accuracy: 0.4927\n",
      "Epoch 39/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.3146 - accuracy: 0.9051 - val_loss: 2.4885 - val_accuracy: 0.4935\n",
      "Epoch 40/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.2871 - accuracy: 0.9135 - val_loss: 2.6002 - val_accuracy: 0.4955\n",
      "Epoch 41/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.2479 - accuracy: 0.9228 - val_loss: 2.6908 - val_accuracy: 0.4946\n",
      "Epoch 42/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.2400 - accuracy: 0.9268 - val_loss: 2.8076 - val_accuracy: 0.4920\n",
      "Epoch 43/45\n",
      "262/262 [==============================] - 2s 8ms/step - loss: 0.2153 - accuracy: 0.9332 - val_loss: 2.8747 - val_accuracy: 0.4898\n",
      "Epoch 44/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.2039 - accuracy: 0.9385 - val_loss: 2.9649 - val_accuracy: 0.4930\n",
      "Epoch 45/45\n",
      "262/262 [==============================] - 2s 7ms/step - loss: 0.1956 - accuracy: 0.9421 - val_loss: 3.0067 - val_accuracy: 0.4926\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 45\n",
    "onecycle = OneCycleScheduler(math.ceil(len(x_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "\n",
    "model_mc_drop.compile(optimizer=optimizers.SGD(lr=0.001, momentum = 0.9), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model_mc_drop.fit(x_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(x_val_scaled, y_val),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is training at an average of 2 seconds per epoch, and performs just as well, if not better, than the previous models. It has a larger batch size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
